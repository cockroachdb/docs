---
title: Technical Advisory 131639
advisory: A-131639
summary: During a sustained period of disk slowness in the presence of lease transfers, it is possible for some writes in a transaction that straddle multiple ranges to be lost.
toc: true
affected_versions: v23.1.0 to v23.1.26, v23.2.0 to v23.2.10, v24.1.0
advisory_date: 2024-10-03
docs_area: releases
---

Publication date: {{ page.advisory_date | date: "%B %e, %Y" }}

## Description

Since v22.2, through v24.2, in circumstances where there is a sustained period of disk slowness in the presence of lease transfers, it is possible for some writes in a transaction that straddle multiple ranges to be lost. In cases where these lost writes are not also present on one or more secondary indexes that could allow for reconstructing or repairing the row, these writes could be irrecoverably lost.

However, in certain scenarios, these writes may be fully recoverable. For example, if the lost writes were on a secondary index, these could be fully repaired from the primary index, or a primary index could be fully repaired if there exists a set of secondary indexes that fully covers the primary index columns. The lost writes may also be partially recoverable. For example, if only some of the lost writes from a primary index are contained in secondary indexes.

In detail, since v22.2, when a lease for a range is transferred, it is done so as an expiration-based lease. This lease is then promoted to an epoch-based lease. In situations where the block device backing a store is experiencing intermittent slowness over an extended period of time, the expiration time for the lease transferred to the node with the slow stores can move back in time during this promotion. The lease expiration is said to have "regressed". This lease expiration regression can result in a brief period of time where another node can claim the lease, and the range can have two leaseholders.

While transactions will typically be routed from gateway nodes to the true leaseholder (on the node with the healthy store), and write intents will be written on this true leaseholder, on the node with the slow store these write intents are still queued up in the range's Raft log, and have not been applied. If async intent resolution (the means by which intents are committed to durable records, once a transaction commits) is subsequently triggered on this slow node before it has caught up on its Raft log, intent resolution becomes a no-op, instead of marking the intent as committed. A future transaction that interacts with the row may then come across this intent, and if the committed transaction's record has been GC-ed at that point, it may incorrectly determine that the intent belongs to an aborted transaction. This would then cause the intent to be cleaned up, effectively dropping the committed write.

This bug is rare in that it requires intermittent slowness of a block device backing a store, coupled with lease transfers onto the store with the intermittently slow block device, followed by async intent resolution on the slow node before it has caught up on its Raft log. Writes must touch more than one range. Single range writes, including those that benefit from the One-Phase-Commit (1PC) fast path are not affected.

All CockroachDB versions prior to and including the following are impacted by this bug:

- v23.1.0 - v23.1.26 and below
- v23.2.0 - v23.2.10 and below
- v24.1.0 - subsequent patch versions are unaffected

Versions prior to 23.1 are no longer eligible for [maintenance support]({% link releases/release-support-policy.md %}), and this issue will not be addressed in those versions.

## Statement

This is resolved in CockroachDB by [#123442](https://github.com/cockroachdb/cockroach/pull/123442) which resolves an issue in the expiration-to-epoch lease promotion transition, where the lease's effective expiration could be allowed to regress, resulting in two nodes believing they are the leaseholder for a range.

The patch has been applied to maintenance releases of CockroachDB:

- [v23.1.27]({% link releases/v23.1.md%}#v23-1-27)
- [v23.2.11]({% link releases/v23.2.md%}#v23-2-11)
- [v24.1.1]({% link releases/v24.1.md%}#v24-1-1)

This public issue is tracked by [131639](https://github.com/cockroachdb/cockroach/issues/131639).

## Mitigation

Users are encouraged to upgrade to [v23.1.27]({% link releases/v23.1.md%}#v23-1-27), [v23.2.11]({% link releases/v23.2.md%}#v23-2-11), [v24.1.1]({% link releases/v24.1.md%}#v24-1-1), or a later version that includes the patch.

### Detection via logs

While log lines alone are insufficient to confirm the occurrence of a lost write, they can be used to identify whether such a situation could have possibly occurred.

First, look for log lines that indicate intermittent slowness of a block device backing one or more stores. This can be seen with log lines like:

~~~
err=heartbeat failed on epoch increment
~~~

Followed by a log line like:

~~~
incremented n.* liveness epoch to .*
~~~

Next, during the same time period as the slowness, look for delayed lease promotions on the same node as the slow disk:

~~~
applied lease after .* replication lag, client traffic may have been delayed.*epo=.*prev.*exp=.*Request.*
~~~

If all three of these log lines are present and the timestamp of the third log is less than the specified "replication lag" after the timestamp of the second log, then it is possible that a lost write occurred on the range identified in the third log line.

### Detection via an audit of primary and secondary indices

If you suspect that a particular row may have been subject to a lost write, you can query the primary and secondary indexes to confirm that the row exists in both places.

For example, if you suspect that a row in `my_table` with primary key `pk=1` has a lost write, you can run the following queries to confirm:

{% include_cached copy-clipboard.html %}
~~~ sql
SELECT count(*) FROM my_table@primary WHERE pk = 1 AS OF SYSTEM TIME '-10s';
SELECT count(*) FROM my_table@<secondary index 1> WHERE pk = 1 AS OF SYSTEM TIME '-10s';
SELECT count(*) FROM my_table@<secondary index 2> WHERE pk = 1 AS OF SYSTEM TIME '-10s';
-- ...
~~~

If any of these queries return different values for `count`, that likely indicates a lost write.

Please note that the secondary index queries will perform full table scans, which are not recommended if the cluster is already overloaded. The `AS OF SYSTEM TIME` clause can mitigate the impact on foreground traffic, but it won't eliminate it in case of overload.

If you are unsure which rows may have been affected but suspect that your cluster experienced lost writes, please contact the [support team](https://support.cockroachlabs.com).

### Repair rows with lost writes

If you experienced lost writes, it is possible to repair rows by re-inserting data. This will generally be easier if you know all the column values that need to be re-inserted. Please contact the [support team](https://support.cockroachlabs.com) for assistance with repairing rows with lost writes.

## Impact

In circumstances where there is a sustained period of disk slowness in the presence of lease transfers, it is possible for some writes in a transaction that straddle multiple ranges to be lost. In cases where these lost writes are not also present on one or more secondary indexes that could allow for reconstructing or repairing the row, these writes could be irrecoverably lost.

Please reach out to the [support team](https://support.cockroachlabs.com) if more information or assistance is needed.
