---
title: Technical Advisory 131639
advisory: A-131639
summary: During a sustained period of disk slowness in the presence of lease transfers, it is possible for some writes in a transaction that straddle multiple ranges to be lost.
toc: true
affected_versions: v23.1.0 to v23.1.26, v23.2.0 to v23.2.10, v24.1.0
advisory_date: 2024-10-03
docs_area: releases
---

Publication date: {{ page.advisory_date | date: "%B %e, %Y" }}

## Description

Since v22.2, through v24.2, in circumstances where there is a sustained period of disk slowness in the presence of lease transfers, it is possible for some writes in a transaction that straddle multiple ranges to be lost. In cases where these lost writes are not also present on one or more secondary indexes that could allow for reconstructing or repairing the row, these writes could be irrecoverably lost.

However, in certain scenarios, these writes may be fully recoverable. For example, if the lost writes were on a secondary index, these could be fully repaired from the primary index, or a primary index could be fully repaired if there exists a set of secondary indexes that fully covers the primary index columns. The lost writes may also be partially recoverable. For example, if only some of the lost writes from a primary index are contained in secondary indexes.

In detail, since v22.2, when a lease for a range is transferred, it is done so as an expiration-based lease. This lease is then promoted to an epoch-based lease. In situations where the block device backing a store is experiencing intermittent slowness over an extended period of time, the expiration time for the lease transferred to the node with the slow stores can move back in time during this promotion. The lease expiration is said to have "regressed". This lease expiration regression can result in a brief period of time where another node can claim the lease, and the range can have two leaseholders.

While transactions will typically be routed from gateway nodes to the true leaseholder (on the node with the healthy store), and write intents will be written on this true leaseholder, on the node with the slow store these write intents are still queued up in the range's Raft log, and have not been applied. If async intent resolution (the means by which intents are committed to durable records, once a transaction commits) is subsequently triggered on this slow node before it has caught up on its Raft log, intent resolution becomes a no-op, instead of marking the intent as committed. A future transaction that interacts with the row may then come across this intent, and if the committed transaction's record has been garbage collected at that point, it may incorrectly determine that the intent belongs to an aborted transaction. This would then cause the intent to be cleaned up, effectively dropping the committed write.

This bug is rare in that it requires intermittent slowness of a block device backing a store, coupled with lease transfers onto the store with the intermittently slow block device, followed by async intent resolution on the slow node before it has caught up on its Raft log. Writes must touch more than one range. Single range writes, including those that benefit from the One-Phase-Commit (1PC) fast path are not affected.

All CockroachDB versions prior to and including the following are impacted by this bug:

- v23.1.0 - v23.1.26 and below
- v23.2.0 - v23.2.10 and below
- v24.1.0 - subsequent patch versions are unaffected

Versions prior to 23.1 are no longer eligible for [maintenance support]({% link releases/release-support-policy.md %}), and this issue will not be addressed in those versions.

## Statement

[#123442](https://github.com/cockroachdb/cockroach/pull/123442) resolves an issue in CockroachDB in the expiration-to-epoch lease promotion transition, where the lease's effective expiration could be allowed to regress, resulting in two nodes believing they are the leaseholder for a range.

The patch has been applied to maintenance releases of CockroachDB:

- [v23.1.27]({% link releases/v23.1.md%}#v23-1-27)
- [v23.2.11]({% link releases/v23.2.md%}#v23-2-11)
- [v24.1.1]({% link releases/v24.1.md%}#v24-1-1)

This public issue is tracked by [131639](https://github.com/cockroachdb/cockroach/issues/131639).

## Mitigation

Users are encouraged to upgrade to [v23.1.27]({% link releases/v23.1.md%}#v23-1-27), [v23.2.11]({% link releases/v23.2.md%}#v23-2-11), [v24.1.1]({% link releases/v24.1.md%}#v24-1-1), or a later version that includes the patch.

### Detection via logs

Users can run the script included below to detect part of the symptoms of this technical advisory. The script accepts a [debug.zip]({% link v24.3/cockroach-debug-zip.md %}) and determines whether any lease expiration regressions are evident from the logs.

As described previously, a lease expiration regression is only one of the two race conditions needed to encounter a lost write, with the other being a stale read during asynchronous intent resolution on the range that had the lease expiration regression. Consequently, this script does not provide a complete method of detection for a lost write. Running this script may result in a limited number of false positives. For complete confirmation, this script should be combined with the method described in [Detection via an audit of primary and secondary indexes](#detection-via-an-audit-of-primary-and-secondary-indexes).

Users should not run the script on a production system. Instead, users should move the `debug.zip` file elsewhere for analysis and run the script there.

The script requires (and asks for) the following dependencies:

- [ripgrep](https://github.com/BurntSushi/ripgrep#installation)
- [DuckDB](https://duckdb.org/docs/installation)

Example usage:

~~~
➜ ./ta131639_check.sh debug.prod-crdb2.20240922001746.zip
Unzipping debug.prod-crdb2.20240922001746.zip into /tmp/debug ...
Searching for liveness epoch increments ...
Searching for slow lease promotions ...
Querying the logs for lease expiration regressions ...

❌ Lease expiration regressions found. Symptoms detected.

symptoms
node 128 observed possible symptoms at 2024-09-21 23:47:00 on ranges 720864, 711800, 642873, 699079, 644902, 720864, 711800, 642873, 699079, 644902, 725684, 621220, 639436, 703387, 609154, 722603, 607997, 724800, 640176, 646033, 612960, 724978, 725684, 621220, 639436, 703387, 609154, 722603, 607997, 724800, 640176, 646033, 612960, 724978, 720864, 711800, 642873, 699079, 644902, 720864, 711800, 642873, 699079, 644902
node 128 observed possible symptoms at 2024-09-21 23:48:00 on ranges 594984, 594984, 594984, 594984, 585576, 612247, 637536, 643694, 660630, 726662, 727745, 648384, 727219, 724141, 500463, 499850, 724408, 725989, 641867, 546891, 723459, 643393, 650058, 557910, 618784, 723156, 715673, 585576, 612247, 637536, 643694, 660630, 726662, 727745, 648384, 727219, 724141, 500463, 499850, 724408, 725989, 641867, 546891, 723459, 643393, 650058, 557910, 618784, 723156, 715673
node 128 observed possible symptoms at 2024-09-21 23:49:00 on ranges 514934, 714084, 643737, 724828, 589621, 588817, 559421, 726012, 570794, 514934, 714084, 643737, 724828, 589621, 588817, 559421, 726012, 570794, 714084, 643737, 714084, 643737, 502858, 643638, 502858, 643638, 661475, 661405, 714084, 643737, 661475, 661405, 714084, 643737
node 128 observed possible symptoms at 2024-09-21 23:50:00 on ranges 628103, 628103
node 128 observed possible symptoms at 2024-09-21 23:51:00 on ranges 617816, 644856, 614700, 571503, 617816, 644856, 614700, 571503
node 128 observed possible symptoms at 2024-09-21 23:52:00 on ranges 613420, 591272, 500227, 594579, 591262, 591792, 727270, 619441, 621548, 593953, 644334, 580710, 727085, 706761, 613420, 591272, 500227, 594579, 591262, 591792, 727270, 619441, 621548, 593953, 644334, 580710, 727085, 706761, 559421, 595807, 513967, 486745, 571015, 611237, 718693, 726325, 636949, 720755, 493149, 573377, 592808, 635896, 721663, 569135, 697501, 721255, 599599, 727777, 547989, 715170, 573444, 534565, 5, 690974, 597070, 559421, 595807, 513967, 486745, 571015, 611237, 718693, 726325, 636949, 720755, 493149, 573377, 592808, 635896, 721663, 569135, 697501, 721255, 599599, 727777, 547989, 715170, 573444, 534565, 5, 690974, 597070, 556237, 556237, 725991, 725991, 613420, 591272, 500227, 594579, 591262, 591792, 727270, 619441, 621548, 593953, 644334, 580710, 727085, 706761, 613420, 591272, 500227, 594579, 591262, 591792, 727270, 619441, 621548, 593953, 644334, 580710, 727085, 706761
node 128 observed possible symptoms at 2024-09-21 23:54:00 on ranges 727585, 718973, 531989, 596489, 641111, 580509, 646218, 642800, 650058, 727585, 718973, 531989, 596489, 641111, 580509, 646218, 642800, 650058, 661475, 647914, 619898, 560547, 661475, 647914, 619898, 560547, 722897, 543169, 534508, 722897, 543169, 534508
node 128 observed possible symptoms at 2024-09-21 23:56:00 on ranges 530121, 530121
node 128 observed possible symptoms at 2024-09-21 23:57:00 on ranges 719785, 731842, 719785, 731842, 617491, 588556, 730596, 617491, 588556, 730596
node 128 observed possible symptoms at 2024-09-21 23:58:00 on ranges 612378, 569802, 645680, 725230, 725608, 608091, 724656, 722811, 628125, 510073, 636975, 624551, 573869, 608151, 642638, 596107, 612378, 569802, 645680, 725230, 725608, 608091, 724656, 722811, 628125, 510073, 636975, 624551, 573869, 608151, 642638, 596107
~~~

The contents of the script `ta131639_check.sh` are as follows:

~~~ shell
#!/usr/bin/env bash
# Run from within an unzipped debug.zip directory.
set -euo pipefail

# Check if ripgrep is installed.
if ! command -v rg &> /dev/null; then
  echo "Error: ripgrep is not installed. Please install following the instructions in https://github.com/BurntSushi/ripgrep#installation"
  exit 1
fi

# Check if duckdb is installed.
if ! command -v duckdb &> /dev/null; then
  echo "Error: duckdb is not installed. Please install following the instructions in https://duckdb.org/docs/installation"
  exit 1
fi

# Grep the debug.zip for the two queries.
echo "Searching for liveness epoch increments ..."
echo "timestamp,remote_node_id,epoch" > logs_epoch_increment.csv
rg -I -r '$1,$2,$3' 'I([0-9]+ [0-9:.]+).*incremented n(.*) liveness epoch to (.*)' nodes >> logs_epoch_increment.csv || true
if [[ $(wc -l <logs_epoch_increment.csv) -eq 1 ]]; then
    printf "\n✅ No liveness epoch increments found. Symptoms not detected.\n"
    exit 0
fi

echo "Searching for slow lease promotions ..."
echo "timestamp,node_id,range_id,range_span,lease_epoch,lease_proposed,prev_expiration" > logs_slow_lease_promo.csv
rg -I -r '$1,$2,$3,$4,$5,$6,$7' 'W([0-9]+ [0-9:.]+).*,n(\d+),.*,r(\d+)/\d+:(.*),raft\].*client traffic may have been delayed.*epo=(\d+).*pro=(\d+\.\d+).*prev.*exp=(\d+\.\d+).*Request.*' nodes >> logs_slow_lease_promo.csv || true
if [[ $(wc -l <logs_slow_lease_promo.csv) -eq 1 ]]; then
    printf "\n✅ No slow lease promotions found. Symptoms not detected.\n"
    exit 0
fi

# Query the logs using duckdb.
echo "Querying the logs for lease expiration regressions ..."
cat <<EOF > query.sql
SELECT
  'node '
  || node_id
  || ' observed possible symptoms at '
  || lease_proposed_min
  || ' on ranges '
  || string_agg(range_id, ', ') as symptoms
FROM
  (
    WITH
      lease_acqs AS (
        SELECT
          strptime(timestamp, '%y%m%d %H:%M:%S.%n')::timestamp_ns as timestamp,
          node_id,
          range_id,
          range_span,
          lease_epoch,
          timezone('UTC', to_timestamp(lease_proposed)) AS lease_proposed,
          timezone('UTC', to_timestamp(prev_expiration)) AS prev_expiration
        FROM 'logs_slow_lease_promo.csv'
      ),
      epoch_incs AS (
        SELECT
          strptime(timestamp, '%y%m%d %H:%M:%S.%n')::timestamp_ns as timestamp,
          remote_node_id,
          epoch
        FROM 'logs_epoch_increment.csv'
      )
    SELECT *, date_trunc('minute', lease_proposed) AS lease_proposed_min
    FROM lease_acqs JOIN epoch_incs
    -- where the leaseholder's liveness record had its epoch incremented
    ON lease_acqs.node_id = epoch_incs.remote_node_id
    -- and the lease acquisition used an epoch that is less than the one
    -- that was incremented to
    AND lease_acqs.lease_epoch < epoch_incs.epoch
    -- and the lease acquisition began before the epoch increment
    AND lease_acqs.lease_proposed < epoch_incs.timestamp
    -- and the lease acquisition completed after the epoch increment
    AND lease_acqs.timestamp > epoch_incs.timestamp
    -- and the previous lease's expiration was after the epoch increment
    AND lease_acqs.prev_expiration > epoch_incs.timestamp
  )
GROUP BY node_id, lease_proposed_min
ORDER BY lease_proposed_min
EOF
duckdb -list < query.sql > logs_lease_regressions.txt

# Output result.
if [ -s logs_lease_regressions.txt ]; then
    printf "\n❌ Lease expiration regressions found. Symptoms detected.\n\n"
    cat logs_lease_regressions.txt
    exit 1
else
    printf "\n✅ No lease expiration regressions found. Symptoms not detected.\n"
    exit 0
fi
~~~

### Detection via an audit of primary and secondary indexes

If you suspect that a particular row may have been subject to a lost write, you can query the primary and secondary indexes to confirm that the row exists in both places.

For example, if you suspect that a row in `my_table` with primary key `pk=1` has a lost write, you can run the following queries to confirm:

{% include_cached copy-clipboard.html %}
~~~ sql
SELECT count(*) FROM my_table@primary WHERE pk = 1 AS OF SYSTEM TIME '-10s';
SELECT count(*) FROM my_table@<secondary index 1> WHERE pk = 1 AS OF SYSTEM TIME '-10s';
SELECT count(*) FROM my_table@<secondary index 2> WHERE pk = 1 AS OF SYSTEM TIME '-10s';
-- ...
~~~

If any of these queries return different values for `count`, that likely indicates a lost write.

Please note that the secondary index queries will perform full table scans, which are not recommended if the cluster is already overloaded. The `AS OF SYSTEM TIME` clause can mitigate the impact on foreground traffic, but it won't eliminate it in case of overload.

If you are unsure which rows may have been affected but suspect that your cluster experienced lost writes, please contact the [support team](https://support.cockroachlabs.com).

### Repair rows with lost writes

If you experienced lost writes, it is possible to repair rows by re-inserting data. This will generally be easier if you know all the column values that need to be re-inserted. Please contact the [support team](https://support.cockroachlabs.com) for assistance with repairing rows with lost writes.

## Impact

In circumstances where there is a sustained period of disk slowness in the presence of lease transfers, it is possible for some writes in a transaction that straddle multiple ranges to be lost. In cases where these lost writes are not also present on one or more secondary indexes that could allow for reconstructing or repairing the row, these writes could be irrecoverably lost.

Please reach out to the [support team](https://support.cockroachlabs.com) if more information or assistance is needed.
