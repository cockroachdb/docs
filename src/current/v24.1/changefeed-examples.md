---
title: Changefeed Examples
summary: Examples for starting and using changefeeds with different aims.
toc: true
docs_area: stream_data
---

This page provides step-by-step examples for using Core and {{ site.data.products.enterprise }} changefeeds. Creating {{ site.data.products.enterprise }} changefeeds is available on CockroachDB {{ site.data.products.dedicated }}, on CockroachDB {{ site.data.products.serverless }}, and with an [{{ site.data.products.enterprise }} license](enterprise-licensing.html) on CockroachDB {{ site.data.products.core }} clusters. Core changefeeds are available in all products.

For a summary of Core and {{ site.data.products.enterprise }} changefeed features, refer to the [Change Data Capture Overview]({% link {{ page.version.version }}/change-data-capture-overview.md %}) page.

{{ site.data.products.enterprise }} changefeeds can connect to the following sinks:

- [Kafka](#create-a-changefeed-connected-to-kafka)
- [Google Cloud Pub/Sub](#create-a-changefeed-connected-to-a-google-cloud-pub-sub-sink)
- [Cloud Storage](#create-a-changefeed-connected-to-a-cloud-storage-sink) (Amazon S3, Google Cloud Storage, Azure Storage)
- {% include_cached new-in.html version="v24.1" %} [Azure Event Hubs](#create-a-changefeed-connected-to-an-azure-event-hubs-sink)
- [Webhook](#create-a-changefeed-connected-to-a-webhook-sink)

Refer to the [Changefeed Sinks]({% link {{ page.version.version }}/changefeed-sinks.md %}) page for more detail on forming sink URIs, available sink query parameters, and specifics on configuration.

{% include {{ page.version.version }}/cdc/recommendation-monitoring-pts.md %}

Use the following filters to show usage examples for either **Enterprise** or **Core** changefeeds:

<div class="filters clearfix">
  <button class="filter-button" data-scope="enterprise">Enterprise Changefeeds</button>
  <button class="filter-button" data-scope="core">Core Changefeeds</button>
</div>

<section class="filter-content" markdown="1" data-scope="enterprise">

{{site.data.alerts.callout_success}}
{% include {{ page.version.version }}/cdc/sink-URI-external-connection.md %}
{{site.data.alerts.end}}

## Create a changefeed connected to Kafka

{{site.data.alerts.callout_info}}
[`CREATE CHANGEFEED`]({% link {{ page.version.version }}/create-changefeed.md %}) is an [{{ site.data.products.enterprise }}-only]({% link {{ page.version.version }}/enterprise-licensing.md %}) feature. For the Core version, see [the `CHANGEFEED FOR` example](#create-a-core-changefeed).
{{site.data.alerts.end}}

In this example, you'll set up a changefeed for a single-node cluster that is connected to a Kafka sink. The changefeed will watch two tables.

1. If you do not already have one, [request a trial {{ site.data.products.enterprise }} license]({% link {{ page.version.version }}/enterprise-licensing.md %}).

1. Use the [`cockroach start-single-node`]({% link {{ page.version.version }}/cockroach-start-single-node.md %}) command to start a single-node cluster:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    cockroach start-single-node --insecure --listen-addr=localhost
    ~~~

1. Download and extract the [Confluent Open Source platform](https://www.confluent.io/download/) (which includes Kafka).

1. In a new terminal window, go to the extracted `confluent-<version>` directory and start Confluent:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    ./bin/confluent local services start
    ~~~

    Only `zookeeper` and `kafka` are needed. To troubleshoot Confluent, see [their docs](https://docs.confluent.io/current/installation/installing_cp.html#zip-and-tar-archives) and the [Quick Start Guide](https://docs.confluent.io/platform/current/quickstart/ce-quickstart.html#ce-quickstart).

1. Create two Kafka topics:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    ./bin/kafka-topics \
    --create \
    --zookeeper localhost:2181 \
    --replication-factor 1 \
    --partitions 1 \
    --topic office_dogs
    ~~~

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    ./bin/kafka-topics \
    --create \
    --zookeeper localhost:2181 \
    --replication-factor 1 \
    --partitions 1 \
    --topic employees
    ~~~

    {{site.data.alerts.callout_info}}
    You are expected to create any Kafka topics with the necessary number of replications and partitions. [Topics can be created manually](https://kafka.apache.org/documentation/#basic_ops_add_topic) or [Kafka brokers can be configured to automatically create topics](https://kafka.apache.org/documentation/#topicconfigs) with a default partition count and replication factor.
    {{site.data.alerts.end}}

{% include {{ page.version.version }}/cdc/sql-cluster-settings-example.md %}

{% include {{ page.version.version }}/cdc/create-example-db-cdc.md %}

1. Start the changefeed:

    {% include_cached copy-clipboard.html %}
    ~~~ sql
    CREATE CHANGEFEED FOR TABLE office_dogs, employees INTO 'kafka://localhost:9092';
    ~~~
    ~~~

            job_id
    +--------------------+
      360645287206223873
    (1 row)

    NOTICE: changefeed will emit to topic office_dogs
    NOTICE: changefeed will emit to topic employees
    ~~~

    This will start up the changefeed in the background and return the `job_id`. The changefeed writes to Kafka.

1. In a new terminal, move into the extracted `confluent-<version>` directory and start watching the Kafka topics:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    ./bin/kafka-console-consumer \
    --bootstrap-server=localhost:9092 \
    --from-beginning \
    --include 'office_dogs|employees'
    ~~~

    ~~~
    {"after": {"id": 1, "name": "Petee H"}}
    {"after": {"id": 2, "name": "Carl"}}
    {"after": {"id": 1, "name": "Lauren", "rowid": 528514320239329281}}
    {"after": {"id": 2, "name": "Spencer", "rowid": 528514320239362049}}
    ~~~

    The initial scan displays the state of the tables as of when the changefeed started (therefore, the initial value of `"Petee"` is omitted).

    {% include {{ page.version.version }}/cdc/print-key.md %}

1. Back in the SQL client, insert more data:

    {% include_cached copy-clipboard.html %}
    ~~~ sql
    INSERT INTO office_dogs VALUES (3, 'Ernie');
    ~~~

1. Back in the terminal where you're watching the Kafka topics, the following output has appeared:

    ~~~
    {"after": {"id": 3, "name": "Ernie"}}
    ~~~

1. When you are done, exit the SQL shell (`\q`).

1. To stop `cockroach`:

    Get the process ID of the node:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    ps -ef | grep cockroach | grep -v grep
    ~~~

    ~~~
      501 21766     1   0  6:21PM ttys001    0:00.89 cockroach start-single-node --insecure --listen-addr=localhost
    ~~~

    Gracefully shut down the node, specifying its process ID:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    kill -TERM 21766
    ~~~

    ~~~
    initiating graceful shutdown of server
    server drained and shutdown completed
    ~~~

1. To stop Kafka, move into the extracted `confluent-<version>` directory and stop Confluent:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    ./bin/confluent local services stop
    ~~~

## Create a changefeed connected to Kafka using Avro

{{site.data.alerts.callout_info}}
[`CREATE CHANGEFEED`]({% link {{ page.version.version }}/create-changefeed.md %}) is an [{{ site.data.products.enterprise }}-only]({% link {{ page.version.version }}/enterprise-licensing.md %}) feature. For the Core version, see [the `CHANGEFEED FOR` example](#create-a-core-changefeed-using-avro).
{{site.data.alerts.end}}

In this example, you'll set up a changefeed for a single-node cluster that is connected to a Kafka sink and emits [Avro](https://avro.apache.org/docs/1.8.2/spec.html) records. The changefeed will watch two tables.

1. If you do not already have one, [request a trial {{ site.data.products.enterprise }} license]({% link {{ page.version.version }}/enterprise-licensing.md %}).

1. Use the [`cockroach start-single-node`]({% link {{ page.version.version }}/cockroach-start-single-node.md %}) command to start a single-node cluster:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    cockroach start-single-node --insecure --listen-addr=localhost
    ~~~

1. Download and extract the [Confluent Open Source platform](https://www.confluent.io/download/) (which includes Kafka).

1. Move into the extracted `confluent-<version>` directory and start Confluent:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    ./bin/confluent local services start
    ~~~

    Only `zookeeper`, `kafka`, and `schema-registry` are needed. To troubleshoot Confluent, see [their docs](https://docs.confluent.io/current/installation/installing_cp.html#zip-and-tar-archives) and the [Quick Start Guide](https://docs.confluent.io/platform/current/quickstart/ce-quickstart.html#ce-quickstart).

1. Create two Kafka topics:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    ./bin/kafka-topics \
    --create \
    --zookeeper localhost:2181 \
    --replication-factor 1 \
    --partitions 1 \
    --topic office_dogs
    ~~~

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    ./bin/kafka-topics \
    --create \
    --zookeeper localhost:2181 \
    --replication-factor 1 \
    --partitions 1 \
    --topic employees
    ~~~

    {{site.data.alerts.callout_info}}
    You are expected to create any Kafka topics with the necessary number of replications and partitions. [Topics can be created manually](https://kafka.apache.org/documentation/#basic_ops_add_topic) or [Kafka brokers can be configured to automatically create topics](https://kafka.apache.org/documentation/#topicconfigs) with a default partition count and replication factor.
    {{site.data.alerts.end}}

{% include {{ page.version.version }}/cdc/sql-cluster-settings-example.md %}

{% include {{ page.version.version }}/cdc/create-example-db-cdc.md %}

1. Start the changefeed:

    {% include_cached copy-clipboard.html %}
    ~~~ sql
    CREATE CHANGEFEED FOR TABLE office_dogs, employees INTO 'kafka://localhost:9092' WITH format = avro, confluent_schema_registry = 'http://localhost:8081';
    ~~~

    {% include {{ page.version.version }}/cdc/confluent-cloud-sr-url.md %}

    ~~~
            job_id
    +--------------------+
      360645287206223873
    (1 row)

    NOTICE: changefeed will emit to topic office_dogs
    NOTICE: changefeed will emit to topic employees
    ~~~

    This will start up the changefeed in the background and return the `job_id`. The changefeed writes to Kafka.

1. In a new terminal, move into the extracted `confluent-<version>` directory and start watching the Kafka topics:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    ./bin/kafka-avro-console-consumer \
    --bootstrap-server=localhost:9092 \
    --from-beginning \
    --include 'office_dogs|employees'
    ~~~

    ~~~
    {"after":{"office_dogs":{"id":{"long":1},"name":{"string":"Petee H"}}}}
    {"after":{"office_dogs":{"id":{"long":2},"name":{"string":"Carl"}}}}
    {"after":{"employees":{"dog_id":{"long":1},"employee_name":{"string":"Lauren"},"rowid":{"long":528537452042682369}}}}
    {"after":{"employees":{"dog_id":{"long":2},"employee_name":{"string":"Spencer"},"rowid":{"long":528537452042747905}}}}
    ~~~

    The initial scan displays the state of the table as of when the changefeed started (therefore, the initial value of `"Petee"` is omitted).

    {% include {{ page.version.version }}/cdc/print-key.md %}

1. Back in the SQL client, insert more data:

    {% include_cached copy-clipboard.html %}
    ~~~ sql
    INSERT INTO office_dogs VALUES (3, 'Ernie');
    ~~~

1. Back in the terminal where you're watching the Kafka topics, the following output has appeared:

    ~~~
    {"after":{"office_dogs":{"id":{"long":3},"name":{"string":"Ernie"}}}}
    ~~~

1. When you are done, exit the SQL shell (`\q`).

1. To stop `cockroach`:

    Get the process ID of the node:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    ps -ef | grep cockroach | grep -v grep
    ~~~

    ~~~
      501 21766     1   0  6:21PM ttys001    0:00.89 cockroach start-single-node --insecure --listen-addr=localhost
    ~~~

    Gracefully shut down the node, specifying its process ID:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    kill -TERM 21766
    ~~~

    ~~~
    initiating graceful shutdown of server
    server drained and shutdown completed
    ~~~

1. To stop Kafka, move into the extracted `confluent-<version>` directory and stop Confluent:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    ./bin/confluent local services stop
    ~~~

## Create a changefeed connected to a Confluent Cloud sink

In this example, you'll set up a changefeed for a single-node cluster that is connected to a Confluent Cloud managed Kafka cluster. The changefeed will watch a table and send messages to the sink.

{% include {{ page.version.version }}/cdc/examples-license-workload.md %}

{% include {{ page.version.version }}/cdc/sql-cluster-settings-example.md %}

1. To prepare the Confluent Cloud Kafka sink, sign up on the [Confluent Cloud trial page](https://www.confluent.io/confluent-cloud/tryfree/).

1. On the **Create cluster** page, select the necessary cluster type. The **Basic** cluster type is sufficient to run this example. Select the configuration and regions. After you have confirmed payment and set a cluster name, **Launch** the cluster.

1. From the **Overview** page, click on **API Keys** in the navigation menu. Click **Create key** and select the scope for this API key. **Global access** is sufficient for this example. **Granular access** is more suitable for production. Copy or download the key and secret.

1. Create a topic in Confluent Cloud. Under **Topics** select **Add topic**. Add a topic name and define the number of partitions and then **Create**. CockroachDB defaults to using the table name as the topic name. If you would like to send messages to an alternate topic, you can specify the [`topic_name`]({% link {{ page.version.version }}/create-changefeed.md %}#topic-name-param) parameter.

    {{site.data.alerts.callout_info}}
    You can enable [auto topic creation](https://docs.confluent.io/cloud/current/clusters/broker-config.html) for Confluent Cloud Dedicated clusters under **Cluster Settings** in the console.
    {{site.data.alerts.end}}

1. Construct the URI to connect your changefeed to the Confluent Cloud cluster. You will need:
    - The prefix scheme `confluent-cloud://`.
    - The address and port of the bootstrap server. Click on **Cluster settings** in your Confluent Cloud console. Under **Endpoints**, you will find the **Bootstrap server**. It will be something like: `pkc-lzvrd.us-west4.gcp.confluent.cloud:9092`.
    - Ensure that you follow the cluster address with `?` before the remaining parameters, and also connect the following parameters with `&`.
    - Your API key and secret from the previous step passed with the `api_key` and `api_secret` parameters. You must [URL encode](https://www.urlencoder.org/) the secret before adding to the connection string.
    - (Optional) Any further parameters. Refer to [Query parameters]({% link {{ page.version.version }}/create-changefeed.md %}#query-parameters).

        ~~~
        'confluent-cloud://pkc-lzvrd.us-west4.gcp.confluent.cloud:9092?api_key={API key}&api_secret={url-encoded API secret}'
        ~~~

1. Back in the SQL shell, create a changefeed:

    {% include_cached copy-clipboard.html %}
    ~~~ sql
    CREATE CHANGEFEED FOR TABLE movr.users, movr.vehicles INTO 'confluent-cloud://pkc-lzvrd.us-west4.gcp.confluent.cloud:9092?api_key={API key}&api_secret={url-encoded API secret}&topic_name=users_and_vehicles' WITH resolved;
    ~~~
    ~~~
            job_id
    ----------------------
    913248812656525313
    (1 row)

    NOTICE: changefeed will emit to topic users_and_vehicles
    ~~~

## Create a changefeed connected to a Google Cloud Pub/Sub sink

In this example, you'll set up a changefeed for a single-node cluster that is connected to a [Google Cloud Pub/Sub](https://cloud.google.com/pubsub/docs/overview) sink. The changefeed will watch a table and send messages to the sink.

You'll need access to a [Google Cloud Project](https://cloud.google.com/resource-manager/docs/creating-managing-projects) to set up a Pub/Sub sink. In this example, the [Google Cloud CLI](https://cloud.google.com/sdk/docs/install-sdk) (`gcloud`) is used, but you can also complete each of these steps within your [Google Cloud Console](https://cloud.google.com/storage/docs/cloud-console).

{% include {{ page.version.version }}/cdc/examples-license-workload.md %}

{% include {{ page.version.version }}/cdc/sql-cluster-settings-example.md %}

1. Next, you'll prepare your Pub/Sub sink.

    In a new terminal window, create a [Service Account](https://cloud.google.com/iam/docs/understanding-service-accounts) attached to your Google Project:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    gcloud iam service-accounts create cdc-demo --project cockroach-project
    ~~~

    In this example, `cdc-demo` will represent the name of the service account, and `cockroach-project` is the name of the Google Project.

    To ensure that your Service Account has the correct permissions to publish to the sink, use the following command to give the Service Account the predefined [Pub/Sub Editor](https://cloud.google.com/iam/docs/understanding-roles#pub-sub-roles) role:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    gcloud projects add-iam-policy-binding cockroach-project --member='serviceAccount:cdc-demo@cockroach-project.iam.gserviceaccount.com' --role='roles/pubsub.editor'
    ~~~

1. Create the Pub/Sub [topic]({% link {{ page.version.version }}/changefeed-sinks.md %}#pub-sub-topic-naming) to which your changefeed will emit messages:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    gcloud pubsub topics create movr-users --project cockroach-project
    ~~~

    Run the following command to create a subscription within the `movr-users` topic:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    gcloud pubsub subscriptions create movr-users-sub --topic=movr-users --topic-project=cockroach-project
    ~~~

1. With the topic and subscription set up, you can now download your Service Account's key. Use the following command to specify where to download the json key file (`key.json`):

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    gcloud iam service-accounts keys create key.json --iam-account=cdc-demo@cockroach-project.iam.gserviceaccount.com
    ~~~

    Next, base64 encode your credentials key using the command specific to your platform.

    If you're working on macOS:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    cat key.json | base64
    ~~~

    If you're working on Linux, run the following to ensure that lines are not wrapped in the output:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    cat key.json | base64 -w 0
    ~~~

    Copy the output so that you can add it to your [`CREATE CHANGEFEED`]({% link {{ page.version.version }}/create-changefeed.md %}) statement in the next step. When you create your changefeed, it is necessary that the key is base64 encoded before passing it in the URI.

1. Back in the SQL shell, create a changefeed that will emit messages to your Pub/Sub topic. Ensure that you pass the base64-encoded credentials for your Service Account:

    {% include_cached copy-clipboard.html %}
    ~~~ sql
    CREATE CHANGEFEED FOR TABLE users INTO 'gcpubsub://cockroach-project?region=us-east1&topic_name=movr-users&AUTH=specified&CREDENTIALS={base64-encoded key}';
    ~~~

    You can include the `region` parameter for your topic, or use the [WITH `unordered`]({% link {{ page.version.version }}/create-changefeed.md %}#unordered) option for multi-region Pub/Sub. See the [Changefeed Sinks]({% link {{ page.version.version }}/changefeed-sinks.md %}#google-cloud-pub-sub) page for more detail.

    The output will confirm the topic where the changefeed will emit messages to.

    ~~~
           job_id
    ----------------------
    756641304964792321
    (1 row)

    NOTICE: changefeed will emit to topic movr-users
    ~~~

    To view all the messages delivered to your topic, you can use:
    - The Google Cloud Console. From the Pub/Sub menu, select **Subscriptions** in the left-hand navigation and then select the subscription ID from your list of subscriptions. On the subscription's overview, click **Messages**, and then **Pull** to view messages.
    - The `gcloud` CLI. From your terminal, run the following command:

        {% include_cached copy-clipboard.html %}
        ~~~ shell
        gcloud pubsub subscriptions pull movr-users-sub --auto-ack --limit=10
        ~~~

        This command will **only** pull these messages once per subscription. For example, if you ran this command again you would receive 10 different messages in your output. To receive more than one message at a time, pass the `--limit` flag. For more details, refer to the [gcloud pubsub subscriptions pull](https://cloud.google.com/sdk/gcloud/reference/pubsub/subscriptions/pull) documentation.

        ~~~
        ┌─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬───────────────────┬──────────────┬────────────┬──────────────────┬────────────┐
        │                                                                                                                                     DATA                                                                                                                                    │     MESSAGE_ID    │ ORDERING_KEY │ ATTRIBUTES │ DELIVERY_ATTEMPT │ ACK_STATUS │
        ├─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼───────────────────┼──────────────┼────────────┼──────────────────┼────────────┤
        │ {"Key":["amsterdam", "09ee2856-5856-40c4-85d3-7d65bed978f0"],"Value":{"after": {"address": "84579 Peter Divide Apt. 47", "city": "amsterdam", "credit_card": "0100007510", "id": "09ee2856-5856-40c4-85d3-7d65bed978f0", "name": "Timothy Jackson"}},"Topic":"movr-users"}       │ 11249015757941393 │              │            │                  │ SUCCESS    │
        │ {"Key":["new york", "8803ab9e-5001-4994-a2e6-68d587f95f1d"],"Value":{"after": {"address": "37546 Andrew Roads Apt. 68", "city": "new york", "credit_card": "4731676650", "id": "8803ab9e-5001-4994-a2e6-68d587f95f1d", "name": "Susan Harrington"}},"Topic":"movr-users"}        │ 11249015757941394 │              │            │                  │ SUCCESS    │
        │ {"Key":["seattle", "32e27201-ca0d-4a0c-ada2-fbf47f6a4711"],"Value":{"after": {"address": "86725 Stephen Gardens", "city": "seattle", "credit_card": "3639690115", "id": "32e27201-ca0d-4a0c-ada2-fbf47f6a4711", "name": "Brad Hill"}},"Topic":"movr-users"}                      │ 11249015757941395 │              │            │                  │ SUCCESS    │
        │ {"Key":["san francisco", "27b03637-ef9f-49a0-9b58-b16d7a9e34f4"],"Value":{"after": {"address": "85467 Tiffany Field", "city": "san francisco", "credit_card": "0016125921", "id": "27b03637-ef9f-49a0-9b58-b16d7a9e34f4", "name": "Mark Garcia"}},"Topic":"movr-users"}          │ 11249015757941396 │              │            │                  │ SUCCESS    │
        │ {"Key":["rome", "982e1863-88d4-49cb-adee-0a35baae7e0b"],"Value":{"after": {"address": "54918 Sutton Isle Suite 74", "city": "rome", "credit_card": "6015706174", "id": "982e1863-88d4-49cb-adee-0a35baae7e0b", "name": "Kimberly Nichols"}},"Topic":"movr-users"}                │ 11249015757941397 │              │            │                  │ SUCCESS    │
        │ {"Key":["washington dc", "7b298994-7b12-414c-90ef-353c7105f012"],"Value":{"after": {"address": "45205 Romero Ford Apt. 86", "city": "washington dc", "credit_card": "3519400314", "id": "7b298994-7b12-414c-90ef-353c7105f012", "name": "Taylor Bullock"}},"Topic":"movr-users"} │ 11249015757941398 │              │            │                  │ SUCCESS    │
        │ {"Key":["boston", "4f012f57-577b-4853-b5ab-0d79d0df1369"],"Value":{"after": {"address": "15765 Vang Ramp", "city": "boston", "credit_card": "6747715133", "id": "4f012f57-577b-4853-b5ab-0d79d0df1369", "name": "Ryan Garcia"}},"Topic":"movr-users"}                            │ 11249015757941399 │              │            │                  │ SUCCESS    │
        │ {"Key":["seattle", "9ba85917-5545-4674-8ab2-497fa47ac00f"],"Value":{"after": {"address": "24354 Whitney Lodge", "city": "seattle", "credit_card": "8642661685", "id": "9ba85917-5545-4674-8ab2-497fa47ac00f", "name": "Donald Walsh"}},"Topic":"movr-users"}                     │ 11249015757941400 │              │            │                  │ SUCCESS    │
        │ {"Key":["seattle", "98312fb3-230e-412d-9b22-074ec97329ff"],"Value":{"after": {"address": "72777 Carol Shoal", "city": "seattle", "credit_card": "7789799678", "id": "98312fb3-230e-412d-9b22-074ec97329ff", "name": "Christopher Davis"}},"Topic":"movr-users"}                  │ 11249015757941401 │              │            │                  │ SUCCESS    │
        └─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴───────────────────┴──────────────┴────────────┴──────────────────┴────────────┘
        ~~~

## Create a changefeed connected to a cloud storage sink

{{site.data.alerts.callout_info}}
[`CREATE CHANGEFEED`]({% link {{ page.version.version }}/create-changefeed.md %}) is an [{{ site.data.products.enterprise }}-only]({% link {{ page.version.version }}/enterprise-licensing.md %}) feature. For the Core version, see [the `CHANGEFEED FOR` example](#create-a-core-changefeed).
{{site.data.alerts.end}}

In this example, you'll set up a changefeed for a single-node cluster that is connected to an AWS S3 sink. The changefeed watches two tables. Note that you can set up changefeeds for any of [these cloud storage providers]({% link {{ page.version.version }}/changefeed-sinks.md %}#cloud-storage-sink).

1. If you do not already have one, [request a trial {{ site.data.products.enterprise }} license]({% link {{ page.version.version }}/enterprise-licensing.md %}).

1. Use the [`cockroach start-single-node`]({% link {{ page.version.version }}/cockroach-start-single-node.md %}) command to start a single-node cluster:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    $ cockroach start-single-node --insecure --listen-addr=localhost
    ~~~

{% include {{ page.version.version }}/cdc/sql-cluster-settings-example.md %}

{% include {{ page.version.version }}/cdc/create-example-db-cdc.md %}

1. Start the changefeed:

    {% include_cached copy-clipboard.html %}
    ~~~ sql
    > CREATE CHANGEFEED FOR TABLE office_dogs, employees INTO 's3://example-bucket-name/test?AWS_ACCESS_KEY_ID=enter_key-here&AWS_SECRET_ACCESS_KEY=enter_key_here' with updated, resolved='10s';
    ~~~

    ~~~
            job_id
    +--------------------+
      360645287206223873
    (1 row)
    ~~~

    This will start up the changefeed in the background and return the `job_id`. The changefeed writes to AWS.

1. Monitor your changefeed on the <a href="http://localhost:8080/#/metrics/changefeeds/cluster" data-proofer-ignore>DB Console</a>. For more information, see [Changefeeds Dashboard]({% link {{ page.version.version }}/ui-cdc-dashboard.md %}).

1. When you are done, exit the SQL shell (`\q`).

1. To stop `cockroach`:

    Get the process ID of the node:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    ps -ef | grep cockroach | grep -v grep
    ~~~

    ~~~
      501 21766     1   0  6:21PM ttys001    0:00.89 cockroach start-single-node --insecure --listen-addr=localhost
    ~~~

    Gracefully shut down the node, specifying its process ID:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    kill -TERM 21766
    ~~~

    ~~~
    initiating graceful shutdown of server
    server drained and shutdown completed
    ~~~

## Create a changefeed connected to an Azure Event Hubs sink

{% include_cached new-in.html version="v24.1" %} In this example, you'll set up a changefeed for a single-node cluster that is connected to [Azure Event Hubs](https://azure.microsoft.com/en-us/products/event-hubs). The changefeed watches two tables. You'll need access to the Azure Developer Portal, you can sign up for an [Azure free account](https://azure.microsoft.com/en-us/free/).

{% include {{ page.version.version }}/cdc/examples-license-workload.md %}

{% include {{ page.version.version }}/cdc/sql-cluster-settings-example.md %}

1. To prepare the Azure Event Hubs sink, in the Portal, [create a resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/manage-resource-groups-portal#create-resource-groups).

1. Once you have a resource group, select **Event Hubs** in the portal navigation menu and then **+ Create** from the **Event Hubs** page tool bar to create a namespace. Complete the naming details for the namespace and select the resource group you created in the previous step. For **Pricing Tier**, ensure that you select **Standard** or **Premium** (the **Basic** tier does not support Apache Kafka workloads). Complete your configuration options, and then create the namespace.

1. Select **Go to resource** after the deployment is complete. On the namespace overview page, select **+ Event Hub** in the tool bar to create an event hub within this namespace. An Event Hub is equivalent to a Kafka topic. On the creation page, add a name and the configuration for partitions and message retention.

    {{site.data.alerts.callout_info}}
    You can manually create an Event Hub, or Event Hubs will create automatically when you start the changefeed.
    {{site.data.alerts.end}}

1. To send changefeed messages to your Event Hub, you will need a `shared_access_key`. Find **Shared access policies** in the left-hand navigation. Click **+ Add** and provide a name for the policy as well as confirming the permissions. Once created, click on the policy name and then copy the **Primary key** — you will need this to create your changefeed.

1. Build the connection string to your Event Hub. You will need the following:
    - The `azure-event-hub://` URI scheme.
    - The **Host name** from your Event Hubs namespace overview page. It will be something like: `{your-event-hubs-namespace}.servicebus.windows.net`.
    - The port `:9093` for the Kafka protocol.
    - The `shared_access_key_name` parameter with the name of your policy.
    - The `shared_access_key` parameter with your [**URL-encoded**](https://www.urlencoder.org/) primary key.

    {% include {{ page.version.version }}/cdc/azure-event-hubs-uri.md %}

    You can include the [`topic_name`]({% link {{ page.version.version }}/create-changefeed.md %}#topic-name-param) or [`topic_prefix`]({% link {{ page.version.version }}/create-changefeed.md %}#topic-prefix-param) parameters to create and name an Event Hub for emitted messages.

1. Create an [external connection]({% link {{ page.version.version }}/create-external-connection.md %}) to store your connection string:

    {% include_cached copy-clipboard.html %}
    ~~~ sql
    CREATE EXTERNAL CONNECTION eventhub AS 'azure-event-hub://{your-event-hubs-namespace}.servicebus.windows.net:9093?shared_access_key_name={policy-name}&shared_access_key={url-encoded key}';
    ~~~

1. Create the changefeed to your Event Hub:

    {% include_cached copy-clipboard.html %}
    ~~~ sql
    CREATE CHANGEFEED FOR TABLE movr.rides, movr.users INTO 'external://eventhub';
    ~~~
    ~~~
            job_id
    ----------------------
    953466342378274817
    (1 row)

    NOTICE: changefeed will emit to topic rides
    NOTICE: changefeed will emit to topic users
    ~~~

1. To confirm the messages emitted to your Event Hub, navigate to the Event Hub's overview page that contains **Requests**, **Messages**, and **Throughput** counters. View the messages by clicking **Process Data** in the left-hand navigation menu. Then select the **Enable real time insights from events** feature box. On the **Query** page, your messages will load under **Input Preview**.

## Create a changefeed connected to a webhook sink

{{site.data.alerts.callout_info}}
[`CREATE CHANGEFEED`]({% link {{ page.version.version }}/create-changefeed.md %}) is an [{{ site.data.products.enterprise }}-only]({% link {{ page.version.version }}/enterprise-licensing.md %}) feature. For the Core version, see [the `CHANGEFEED FOR` example](#create-a-core-changefeed).
{{site.data.alerts.end}}

In this example, you'll set up a changefeed for a single-node cluster that is connected to a local HTTP server via a webhook. For this example, you'll use an [example HTTP server](https://github.com/cockroachlabs/cdc-webhook-sink-test-server/tree/master/go-https-server) to test out the webhook sink.

1. If you do not already have one, [request a trial {{ site.data.products.enterprise }} license]({% link {{ page.version.version }}/enterprise-licensing.md %}).

1. Use the [`cockroach start-single-node`]({% link {{ page.version.version }}/cockroach-start-single-node.md %}) command to start a single-node cluster:

    {% include_cached copy-clipboard.html %}
    ~~~ shell
    $ cockroach start-single-node --insecure --listen-addr=localhost
    ~~~

1. In this example, you'll run CockroachDB's [Movr]({% link {{ page.version.version }}/movr.md %}) application workload to set up some data for your changefeed.

     First create the schema for the workload:

     {% include_cached copy-clipboard.html %}
     ~~~shell
     cockroach workload init movr "postgresql://root@127.0.0.1:26257?sslmode=disable"
     ~~~

     Then run the workload:

     {% include_cached copy-clipboard.html %}
     ~~~shell
     cockroach workload run movr --duration=1m "postgresql://root@127.0.0.1:26257?sslmode=disable"
     ~~~

{% include {{ page.version.version }}/cdc/sql-cluster-settings-example.md %}

1. In a separate terminal window, set up your HTTP server. Clone the test repository:

    {% include_cached copy-clipboard.html %}
    ~~~shell
    git clone https://github.com/cockroachlabs/cdc-webhook-sink-test-server.git
    ~~~

    {% include_cached copy-clipboard.html %}
    ~~~shell
    cd cdc-webhook-sink-test-server/go-https-server
    ~~~

1. Next make the script executable and then run the server (passing a specific port if preferred, otherwise it will default to `:3000`):

    {% include_cached copy-clipboard.html %}
    ~~~shell
    chmod +x ./server.sh
    ~~~

    {% include_cached copy-clipboard.html %}
    ~~~shell
    ./server.sh <port>
    ~~~

1. Back in your SQL shell, run the following statement to create a changefeed that emits to your webhook sink:

    {% include_cached copy-clipboard.html %}
    ~~~sql
    CREATE CHANGEFEED FOR TABLE movr.vehicles INTO 'webhook-https://localhost:3000?insecure_tls_skip_verify=true' WITH updated;
    ~~~

    You set up a changefeed on the `vehicles` table, which emits changefeed messages to the local HTTP server.

    See the [options table]({% link {{ page.version.version }}/create-changefeed.md %}#options) for more information on the options available for creating your changefeed to a webhook sink.

    ~~~
          job_id
    ----------------------
    687842491801632769
    (1 row)
    ~~~

    In the terminal where your HTTP server is running, you'll receive output similar to:

    ~~~
    2021/08/24 14:00:21 {"payload":[{"after":{"city":"rome","creation_time":"2019-01-02T03:04:05","current_location":"39141 Travis Curve Suite 87","ext":{"brand":"Schwinn","color":"red"},"id":"d7b18299-c0c4-4304-9ef7-05ae46fd5ee1","dog_owner_id":"5d0c85b5-8866-47cf-a6bc-d032f198e48f","status":"in_use","type":"bike"},"key":["rome","d7b18299-c0c4-4304-9ef7-05ae46fd5ee1"],"topic":"vehicles","updated":"1629813621680097993.0000000000"}],"length":1}
    2021/08/24 14:00:22 {"payload":[{"after":{"city":"san francisco","creation_time":"2019-01-02T03:04:05","current_location":"84888 Wallace Wall","ext":{"color":"black"},"id":"020cf7f4-6324-48a0-9f74-6c9010fb1ab4","dog_owner_id":"b74ea421-fcaf-4d80-9dcc-d222d49bdc17","status":"available","type":"scooter"},"key":["san francisco","020cf7f4-6324-48a0-9f74-6c9010fb1ab4"],"topic":"vehicles","updated":"1629813621680097993.0000000000"}],"length":1}
    2021/08/24 14:00:22 {"payload":[{"after":{"city":"san francisco","creation_time":"2019-01-02T03:04:05","current_location":"3893 Dunn Fall Apt. 11","ext":{"color":"black"},"id":"21b2ec54-81ad-4af7-a76d-6087b9c7f0f8","dog_owner_id":"8924c3af-ea6e-4e7e-b2c8-2e318f973393","status":"lost","type":"scooter"},"key":["san francisco","21b2ec54-81ad-4af7-a76d-6087b9c7f0f8"],"topic":"vehicles","updated":"1629813621680097993.0000000000"}],"length":1}
    ~~~

    For more detail on emitted changefeed messages, see [responses]({% link {{ page.version.version }}/changefeed-messages.md %}#responses).

</section>

<section class="filter-content" markdown="1" data-scope="core">

Core changefeeds stream row-level changes to a client until the underlying SQL connection is closed.

## Create a Core changefeed

{% include {{ page.version.version }}/cdc/create-core-changefeed.md %}

## Create a Core changefeed using Avro

{% include {{ page.version.version }}/cdc/create-core-changefeed-avro.md %}

For further information on Core changefeeds, see [`EXPERIMENTAL CHANGEFEED FOR`]({% link {{ page.version.version }}/changefeed-for.md %}).

</section>

## See also

- [`EXPERIMENTAL CHANGEFEED FOR`]({% link {{ page.version.version }}/changefeed-for.md %})
- [`CREATE CHANGEFEED`]({% link {{ page.version.version }}/create-changefeed.md %})
- [Changefeed Messages]({% link {{ page.version.version }}/changefeed-messages.md %})
