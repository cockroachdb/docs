## v23.1.0

Release Date: May 15, 2023

With the release of CockroachDB v23.1, we've added new capabilities in CockroachDB to help you migrate, build, and operate more efficiently. Check out a [summary of the most significant user-facing changes](#v23-1-0-feature-highlights) and then [upgrade to CockroachDB v23.1]({% link v23.1/upgrade-cockroach-version.md %}).

{% include releases/new-release-downloads-docker-image.md release=include.release advisory_key="a103220"%}

<h3 id="v23-1-0-feature-highlights">Feature highlights</h3>

This section summarizes the most significant user-facing changes in v23.1.0. For a complete list of features and changes, including bug fixes and performance improvements, see the [release notes]({% link releases/index.md %}#testing-releases) for previous testing releases. You can also search for [what's new in v23.1 in our docs](https://www.cockroachlabs.com/docs/search?query=new+in+v23.1).

{{site.data.alerts.callout_info}}
The features highlighted below are freely available in CockroachDB {{ site.data.products.core }} and do not require an [enterprise license]({% link v23.1/cockroach-demo.md %}) to test enterprise features in a local, temporary cluster.
{{site.data.alerts.end}}

- [SQL](#v23-1-0-sql)
- [Security and compliance](#v23-1-0-security-and-compliance)
- [Recovery and I/O](#v23-1-0-recovery-and-io)
- [Database operations](#v23-1-0-database-operations)
- [Backward-incompatible changes](#v23-1-0-backward-incompatible-changes)
- [Deprecations](#v23-1-0-deprecations)
- [Known limitations](#v23-1-0-known-limitations)
- [Additional resources](#v23-1-0-additional-resources)

<style>
    table td:first-child {
        min-width: 100px !important;
    }
    table td:nth-child(2) {
        min-width: 200px !important;
    }
</style>

<div id="feature-highlights">

<h4 id="v23-1-0-sql">SQL</h4>

<h5>Queries</h5>

<table>
  <thead>
   <tr>
   <th>Feature</th>
   <th>Description</th>
   </tr>
  </thead>
  <tbody>
  <tr>
   <td><p>Full-text search using TSVector and TSQuery</p>
   </td>
   <td><p>A <a href="https://www.cockroachlabs.com/docs/v23.1/full-text-search">full-text search</a> is used to perform queries on natural-language documents such as articles, websites, or other written formats, with results often sorted by relevance.</p>
    <p>You can rely on <a href="https://www.cockroachlabs.com/docs/v23.1/functions-and-operators#full-text-search-functions">new built-in functions</a> to make use of the new <a href="https://www.cockroachlabs.com/docs/v23.1/tsvector">TSVECTOR</a> and <a href="https://www.cockroachlabs.com/docs/v23.1/tsquery">TSQUERY</a> data types.</p>
   </td>
  </tr>
  <tr>
   <td><p>Improved developer experience for multi-region apps</p>
   </td>
   <td><p>If you have functionality that requires low latency and cannot tolerate delays between regions, you can enable the <code>enforce_home_region</code> option, which <a href="https://www.cockroachlabs.com/docs/v23.1/cost-based-optimizer#control-whether-queries-are-limited-to-a-single-region">ensures that queries are executed within a single region</a>. If a query doesn't have a home region or is running outside of its home region, the optimizer now provides improved feedback and suggestions for executing the query within a single region.
   </td>
  </tr>
  <tr>
   <td><p>Streamline migrations with improved COPY performance</p>
   </td>
   <td><p>Enhancements to the functionality behind COPY statements resulted in 2x faster migrations using AWS DMS.
   </td>
  </tr>
  <tr>
   <td><p>Redact PII from statement bundles</p>
   </td>
   <td><p>Leverage statement bundles for debugging without introducing data privacy concerns. You can now <a href="https://www.cockroachlabs.com/docs/v23.1/explain-analyze#redact-option">redact personally identifiable information (PII) from statement bundles</a> for PCI compliance.
   </td>
  </tr>
  <tr>
   <td><p>User-Defined Function (UDF) enhancements</p>
   </td>
   <td><p><a href="https://www.cockroachlabs.com/docs/v23.1/user-defined-functions">User-defined functions</a> offer enhanced flexibility, performance, and reusability. This release brings a number of UDF enhancements, including: Inlining of supported UDFs within the query plan to improve performance; support for subqueries in statements, support for expressions with a <code>*</code> such as <code>SELECT *</code> , and support for returning a set of results (using <code>SETOF</code>).
<p>
UDFs can now also be used in Changefeed expressions (Enterprise) and <code>CHECK</code> constraints, and referenced from other objects. Validations have been added to guarantee that all statements in the function body should be as strict as the expected UDF volatility. UDFs are also now included in backup and restore operations.
   </td>
  </tr>
  <tr>
   <td><p>DELETE FROM ... USING</p>
   </td>
   <td><p>We have added support for the <a href="https://www.cockroachlabs.com/docs/v23.1/delete#parameters"><code>USING</code> clause on <code>DELETE</code></a>, which allows joining multiple tables for a <code>DELETE</code> clause. This change is in line with PostgreSQL functionality and extends the flexibility of <code>DELETE</code>.
   </td>
  </tr>
  </tbody>
</table>

<h5>Schemas</h5>

<table>
  <thead>
   <tr>
   <th>Feature</th>
   <th>Description</th>
   </tr>
  </thead>
  <tbody>
  <tr>
   <td><p>Support user-defined composite types</p>
   </td>
   <td><p>You can now create your own <a href="https://www.cockroachlabs.com/docs/v23.1/create-type#create-an-enumerated-data-type">enumerated data types</a>.
<p>
For example:<pre>
CREATE TYPE t AS (a INT, b INT);
CREATE TABLE a (a t);
INSERT INTO a VALUES((1, 2))
SELECT (a).b FROM a</pre>
   </td>
  </tr>
  <tr>
   <td><p>Declarative schema changer supports user-defined functions UDFs</p>
   </td>
   <td><p>The statements <code>CREATE FUNCTION</code> and <code>DROP FUNCTION</code> are now supported by the <a href="https://www.cockroachlabs.com/docs/v23.1/online-schema-changes#declarative-schema-changer">declarative schema changer</a>.
   </td>
  </tr>
  <tr>
   <td><p>Declarative schema changer fully supports constraints </p>
   </td>
   <td><p> The statements <code>ALTER TABLE...ADD CONSTRAINT</code> and <code>ALTER TABLE...DROP CONSTRAINT</code> are now supported by the <a href="https://www.cockroachlabs.com/docs/v23.1/online-schema-changes#declarative-schema-changer">declarative schema changer</a>.</td>
  </tr>
  <tr>
   <td><p>Add configurable setting to adjust grant lease options </p>
   </td>
   <td><p>A new session variable <code>allow_role_memberships_to_change_during_transaction</code> has been introduced which, when true, will make granting and revoking of <a href="https://www.cockroachlabs.com/docs/v23.1/security-reference/authorization#roles">role memberships</a> faster at the cost of allowing some in-progress transactions to observe the previous role membership.
<p>
By default, when granting or revoking a role from another role, the system waits until all transactions that are consulting the current set of role memberships to complete. This is done to preserve CockroachDB’s default isolation level. However, the downside of this wait is that grant and revoke will take longer than the longest currently executing transaction.
<p>
In some cases, you may not care about whether concurrent transactions will immediately see the side-effects of the role grant or revoke operation, but would instead prefer that the operation finish quickly.
<p>
For more information about this setting and how it works, see <a href="https://www.cockroachlabs.com/docs/v23.1/grant#limitations"><code>GRANT</code> limitations</a>.
   </td>
  </tr>
  </tbody>
</table>

<h5>Sessions</h5>

<table>
  <thead>
   <tr>
   <th>Feature</th>
   <th>Description</th>
   </tr>
  </thead>
  <tbody>
  <tr>
   <td><p>New SQL shell editor with tab completion</p>
   </td>
   <td><p>The SQL shell has a new user interface that allows <a href="https://www.cockroachlabs.com/docs/v23.1/cockroach-sql#tab-completion">tab completion</a> and more advanced navigation, streamlining developer workflows.
<p>
After pressing tab, you can navigate database objects, keywords, and functions using arrow keys, pressing tab again to select one and return to the console. You can also use pattern matching to filter these entities and find what you need faster.
   </td>
  </tr>
  <tr>
   <td><p>Support multiple active portals (Preview)
</p>
   </td>
   <td><p>The <a href="https://www.cockroachlabs.com/docs/v23.1/cockroachdb-feature-availability#multiple-active-portals">multiple active portals feature of the Postgres wire protocol (pgwire)</a> is available in CockroachDB, with limitations. This allows for more efficient data retrieval by reducing the number of roundtrips required between the client and server.
<p>
Third-party tools such as <a href="https://www.cockroachlabs.com/docs/v23.1/build-a-python-app-with-cockroachdb-asyncpg">asyncpg</a> use this feature to implement efficient asynchronous communication between the PostgreSQL server and client. This can allow for faster, more scalable applications that can handle large amounts of data without slowing down the user experience.
   </td>
  </tr>
  <tr>
   <td><p>Full support for asyncpg </p>
   </td>
   <td><p>CockroachDB now offers <a href="https://www.cockroachlabs.com/docs/v23.1/build-a-python-app-with-cockroachdb-asyncpg">full support for asyncpg</a>, a PostgreSQL database interface library designed specifically for Python's asyncio framework. It provides an efficient implementation of the PostgreSQL server binary protocol for high-performance asynchronous database applications.
<p>
Asyncpg is commonly used with ORM libraries such as SQLAlchemy to provide a simple, flexible, and efficient adapter for working with PostgreSQL databases. This makes it ideal for handling large volumes of data and scaling applications to meet demanding performance requirements.
   </td>
  </tr>
  </tbody>
</table>

<h4 id="v23-1-0-security-and-compliance">Security and compliance</h4>

<table>
  <thead>
   <tr>
   <th>Feature</th>
   <th>Description</th>
   </tr>
  </thead>
  <tbody>
  <tr>
   <td><p>Improvements to the redaction of data in observability artifacts </p>
   </td>
   <td><p>We have made a number of improvements to ensure that sensitive data can be redacted in observability artifacts produced by CockroachDB, such as <a href="https://www.cockroachlabs.com/docs/v23.1/cockroach-debug-zip#redact-sensitive-information">debug.zip</a> and <a href="https://www.cockroachlabs.com/docs/v23.1/explain-analyze#parameters">statement bundles</a>. These improvements, available to all Self-Hosted customers, also help Cockroach Labs to comply with PCI DSS in CockroachDB Dedicated.
   </td>
  </tr>
  <tr>
   <td><p>FIPS-ready CockroachDB binaries </p>
   </td>
   <td><p><a href="https://www.cockroachlabs.com/docs/v23.1/fips">FIPS-ready binaries</a> and Docker images are available for CockroachDB 23.1.0 and above. Federal Information Processing Standards (FIPS) 140-2 is a standard used to approve cryptographic modules by the U.S. and Canadian governments for systems maintained by relevant agencies and organizations working with them for the purposes of encrypting and decrypting sensitive data.
<p>FIPS-ready CockroachDB binaries are designed for workloads that require FIPS 140-2. FIPS-ready CockroachDB delegates cryptographic operations to the OpenSSL library available on the host operating system, rather than Go's cryptographic libraries. We recommend that OpenSSL has a FIPS 140-2 certificate. FIPS mode must be enabled in the Linux kernel to ensure that FIPS 140-2 is enforced by the operating system.
   </td>
  </tr>
  <tr>
   <td><p>Support Oauth authentication protocol for changefeeds </p>
   </td>
   <td><p>See this item in Change Data Capture (Changefeeds).
   </td>
  </tr>
  <tr>
   <td><p>Support encrypted backups with keys stored in Azure Key Vault </p>
   </td>
   <td><p>You can now <a href="https://www.cockroachlabs.com/docs/v23.1/take-and-restore-encrypted-backups">take and restore encrypted backups</a> using RSA keys stored in Azure Key Vault.
   </td>
  </tr>
  <tr>
   <td><p>Expand on External Connections for Changefeeds </p>
   </td>
   <td><p>See the Changefeeds section for more information.
   </td>
  </tr>
  <tr>
   <td><p>New fine-grained system privilege to view all jobs</p>
   </td>
   <td><p>The new <code><a href="https://www.cockroachlabs.com/docs/v23.1/security-reference/authorization#supported-privileges">VIEWJOB system privilege</a></code> allows a user to view all jobs when running commands like <code>SHOW JOBS</code>, without granting additional capabilities. This helps ensure alignment with the principle of least privilege by removing the need to assign broader privileges or role options like <code>CONTROLJOB</code>.
   </td>
  </tr>
  </tbody>
</table>

<h4 id="v23-1-0-recovery-and-io" markdown="1">Recovery and I/O</h4>

<h5>Change data capture (Changefeeds)</h5>

{{site.data.alerts.callout_info}}
The following are [enterprise-only]({% link v23.1/enterprise-licensing.md %}) features. [Request a 30-day trial license](https://www.cockroachlabs.com/get-cockroachdb/enterprise/) to try them out.
{{site.data.alerts.end}}

<table>
  <thead>
   <tr>
   <th>Feature</th>
   <th>Description</th>
   </tr>
  </thead>
  <tbody>
  <tr>
   <td><p>Create scheduled exports using changefeeds </p>
   </td>
   <td><p>Changefeeds can offer benefits over existing export functionality for customers who need higher levels of scale and observability. You can now create <a href="https://www.cockroachlabs.com/docs/v23.1/export-data-with-changefeeds">Changefeeds as Export</a> functionality.
   </td>
  </tr>
  <tr>
   <td><p>Use a webhook as a changefeed sink</p>
   </td>
   <td><p>The use of a <a href="https://www.cockroachlabs.com/docs/v23.1/changefeed-sinks#webhook-sink">webhook sink</a> to deliver changefeed messages to an arbitrary HTTPS endpoint has been promoted from Preview to GA.
   </td>
  </tr>
  <tr>
   <td><p>CDC Queries</p>
   </td>
   <td><p><a href="https://www.cockroachlabs.com/docs/v23.1/cdc-queries">CDC Queries</a> (formerly CDC Transformations) have been promoted from Preview to GA.
   </td>
  </tr>
  <tr>
   <td><p>Use External Connections (GA) to remove a data exfiltration vector</p>
   </td>
   <td><p>Use external connections to specify and interact with resources that are external from CockroachDB. With <a href="https://www.cockroachlabs.com/docs/v23.1/create-external-connection"><code>CREATE EXTERNAL CONNECTION</code></a>, you define a name for an external connection while passing the provider URI and query parameters. <code>BACKUP</code>, <code>RESTORE</code>, <code>IMPORT</code>, <code>EXPORT</code>, and <code>CREATE CHANGEFEED</code> queries can interact with the defined external connection instead of a required, provider-specific URI. As a result, you can decouple the management and permissions of the external resource from the operation in which you're using them.
<p>
With the move from Preview to GA, this feature brings many new capabilities, such as fine-grained permission and support for schema registries, webhook and GC PubSub sinks, and the SHOW command.
   </td>
  </tr>
  <tr>
   <td><p>Changefeed locality </p>
   </td>
   <td><p><code>CREATE CHANGEFEED</code> now accepts a <code>WITH execution_locality</code> option to <a href="https://www.cockroachlabs.com/docs/v23.1/changefeeds-in-multi-region-deployments#run-a-changefeed-job-by-locality">restrict execution of the changefeed process to nodes within the specified locality filter</a>.
   </td>
  </tr>
  <tr>
   <td><p>Improved changefeed resilience </p>
   </td>
   <td><p>Changefeeds are more stable as the result of improved error handling. Changefeeds now default to retrying requests when encountering any error, except those deemed terminal.
</td>
  </tr>
  <tr>
   <td><p>Support Oauth authentication protocol for changefeeds </p>
   </td>
   <td><p><a href="https://www.cockroachlabs.com/docs/v23.1/connect-to-a-changefeed-kafka-sink-with-oauth-using-okta">Oauth authentication</a> uses a third-party software provider to authenticate with Kafka instead of providing CockroachDB with direct access to Kafka cluster credentials. The third-party authentication server provides a temporary credential token that CockroachDB then uses to connect to a customer’s Kafka cluster. This represents a security best practice, allowing users to authenticate without directly storing or sharing their credentials.
   </td>
  </tr>
  </tbody>
</table>

<h5 id="disaster-recovery">Disaster Recovery</h5>

<table>
  <thead>
   <tr>
   <th>Feature</th>
   <th>Description</th>
   </tr>
  </thead>
  <tbody>
  <tr>
   <td><p>Support longer incremental backup chains</p>
   </td>
   <td><p>We now support up to 400 incremental backups, an 8x increase, enabling you to preserve your data even more effectively (reducing RPO) while being more cost-efficient. Incremental backups contain only the data that has changed since the last backup, so they are smaller and faster to produce.
   </td>
  </tr>
  <tr>
   <td><p>Restrict backups to a locality</p>
   </td>
   <td><p>You can now <a href="https://www.cockroachlabs.com/docs/v23.1/take-locality-restricted-backups"restrict backup execution</a> to a specific region/locality. Only the nodes in the specified locality filter will execute the backup, so only those nodes will need access to the backup storage bucket.
<p>
If the node executing the backup does not have a specific range to be backed up, it will read it from the closest replica it can.
   </td>
  </tr>
  <tr>
   <td><p>Support implicit authentication on Azure</p>
   </td>
   <td><p>You can now use implicit authentication on Azure for <a href="https://www.cockroachlabs.com/docs/v23.1/cloud-storage-authentication">cloud storage authentication</a>.
<p>
If the node executing the backup does not have a specific range to be backed up, it will read it from the closest replica it can.
   </td>
  </tr>
  <tr>
   <td><p>Support encrypted backups with keys stored in Azure Key Vault</p>
   </td>
   <td><p>You can now <a href="https://www.cockroachlabs.com/docs/v23.1/take-and-restore-encrypted-backups">take and restore encrypted backups</a> using RSA keys stored in Azure Key Vault.
   </td>
  </tr>
  <tr>
   <td><p>Enforce supported backup versions</p>
   </td>
   <td><p>To help ensure backups and restores are successful, CockroachDB now <a href="https://www.cockroachlabs.com/docs/v23.1/restoring-backups-across-versions">enforces its previous support</a> for restoring backups on a cluster on a specific major version into a cluster that is on the same version or the next major version. Previously, restoring backups produced from even earlier versions was possible, but unreliable. Now, this operation is prevented with an error.
   </td>
  </tr>
  </tbody>
</table>

<h4 id="v23-1-0-database-operations">Database Operations</h4>

<h5>Observability</h5>

<table>
  <thead>
   <tr>
   <th>Feature</th>
   <th>Description</th>
   </tr>
  </thead>
  <tbody>
  <tr>
   <td><p>Key Visualizer (Preview)  </p>
   </td>
   <td><p>Using a visual heatmap of the latest historical range activity across the cluster, you can quickly identify hot spots and full table scans, enabling you to target ranges for performance investigations.
   </td>
  </tr>
  <tr>
   <td><p>Enhanced Intelligent Insights experience includes transaction-level insights and metrics </p>
   </td>
   <td><p>We have expanded the <a href="https://www.cockroachlabs.com/docs/v23.1/ui-insights-page#transaction-executions-view">Insights</a> section of the Console, offering improved discoverability and data for tuning and optimizing your workload. Contention insights reveal the waiting statement for cases where blocking conflicts occur. Transaction-level insights help you identify impacted areas of your application and prioritize your investigations, enabling you to drill down to individual statements with additional insights such as suboptimal plans.
   </td>
  </tr>
  <tr>
   <td><p>Enhanced statement metrics</p>
   </td>
   <td><p>Easily correlate high-level cluster metrics (e.g., CPU, latency, etc.) with per-statement CPU utilization, latency metrics (P50, P90, P99, min, max), idle/client wait time, and MVCC garbage statistics.
   </td>
  </tr>
  <tr>
   <td><p>Faster performance and an enhanced UX for the SQL Activity pages</p>
   </td>
   <td><p>Reliably and quickly find SQL activity information using a new interactive ‘Search Criteria’ capability in the console.
   </td>
  </tr>
  <tr>
   <td><p>Supplement troubleshooting flows with additional observability into indexes used per statement</p>
   </td>
   <td><p>Observability information is available to correlate statements (and their plans) to indexes. You can now map index usage statistics with statements and transactions which streamlines troubleshooting flows such as dropping infrequently used indexes, creating or updating table statistics, reducing MVCC garbage, and alleviating resource hot spots.
   </td>
  </tr>
  </tbody>
</table>

<h5>KV Layer</h5>

<table>
  <thead>
   <tr>
   <th>Feature</th>
   <th>Description</th>
   </tr>
  </thead>
  <tbody>
  <tr>
   <td><p>Decommission Pre-Flight Checks </p>
   </td>
   <td><p><a href="https://www.cockroachlabs.com/docs/v23.1/node-shutdown?filters=decommission">Decommissioning operations</a> now check that each replica on a node that is slated to be decommissioned can be moved to another node.
<p>
Any ranges that are not yet fully upreplicated will block the decommission process.
<p>
When errors are detected that would prevent successful decommissioning,, the errors are printed to STDERR and the decommissioning command exits.
   </td>
  </tr>
  <tr>
   <td><p>Delegated Snapshots: Send Raft snapshots between follower replicas </p>
   </td>
   <td><p><a href="https://www.cockroachlabs.com/docs/v23.1/architecture/replication-layer#snapshots">Delegated Snapshots</a> make multi-region deployments more cost-efficient by decreasing the use of remote snapshots if there is a local snapshot with the data.
<p>
Sending data locally reduces your network costs and frees up the WAN bandwidth for the important data that must be transferred
<p>
Previously, customers with multi-site deployments paid more for network bandwidth than strictly necessary due to system operations that required snapshots.  Additionally, because of congested TCP channels between regions, transferring snapshots could negatively impact user traffic and slow snapshot transfers.
<p>
Delegated Snapshots fixes this problem by sending snapshots from a local replica whenever a close-by replica exists and isn’t busy.
   </td>
  </tr>
  <tr>
   <td><p>Faster leaseholder recovery </p>
   </td>
   <td><p>The default CockroachDB lease duration has been reduced from 9 seconds to 6 seconds, to reduce range unavailability following leaseholder loss. Some <a href="https://www.cockroachlabs.com/docs/v23.1/architecture/replication-layer#important-values-and-timeouts">other related settings</a> have also had their defaults reduced, including heartbeat intervals, Raft election timeouts, and network timeouts.
   </td>
  </tr>
  <tr>
   <td><p>Lower default TTL for garbage collection (GC)</p>
   </td>
   <td><p>The default <a href="https://www.cockroachlabs.com/docs/v23.1/configure-replication-zones#gc-ttlseconds">GC TTL value</a> in 23.1.0 is being reduced from 25 hrs to 4 hrs for new clusters.
   <p>
This change is being made to improve read performance, storage utilization, and cluster stability in high write traffic scenarios.
<a href="https://www.cockroachlabs.com/docs/v23.1/manage-a-backup-schedule">Scheduled backups</a> will not be affected by this change as <a href="https://www.cockroachlabs.com/docs/v23.1/architecture/storage-layer#protected-timestamps">protected timestamps</a> will ensure data isn't garbage-collected until it has been backed up. <a href="https://www.cockroachlabs.com/docs/v23.1/create-changefeed">Changefeeds</a> will also not be affected. If you want a 25-hour or larger GC TTL value (for example, to support <a href="https://www.cockroachlabs.com/docs/v23.1/as-of-system-time">AS OF SYSTEM TIME</a> queries that go further back in time), you can explicitly set GC TTL to the desired value.
<p>
This change will only apply to new clusters. Existing clusters will retain the 25-hour default when upgrading, unless you have previously overridden it with an explicit value, in which case, that value will be retained. Backups taken from clusters running versions prior to v23.1 will similarly retain the GC TTL configured when the backup was taken.
   </td>
  </tr>
  </tbody>
</table>

</div>

<h4 id="v23-1-0-backward-incompatible-changes">Backward-incompatible changes</h4>

Before [upgrading to CockroachDB v23.1]({% link v23.1/upgrade-cockroach-version.md %}), be sure to review the following backward-incompatible changes, as well as key cluster setting changes, and adjust your deployment as necessary.

- Replaced the `cdc_prev()` [function]({% link v23.1/functions-and-operators.md %}) in favor of a `cdc_prev` tuple. This is an incompatible change that may break [changefeeds]({% link v23.1/change-data-capture-overview.md %}) that use the previous `cdc_prev()` function. [#85177][#85177]
- [`SHOW RANGES FOR TABLE`]({% link v23.1/show-ranges.md %}) now includes rows for all indexes that support the table. Prior to this change, `SHOW RANGES FOR TABLE foo` was an alias for `SHOW RANGES FOR INDEX foo@primary`. This was causing confusion, as it would miss data for secondary indexes. It is still possible to filter to just the primary index using `SHOW RANGES FOR INDEX foo@primary`. The statement output now also includes the index name. [#93545][#93545]
- CockroachDB now supports sharing storage ranges across multiple indexes/tables. This behavior is not enabled by default in v23.1, but will be enabled by default in a future release. When the behavior is enabled, there will no longer be a guarantee that there is at most one SQL object (e.g., table/index/sequence/materialized view) per storage range. As a result, the columns `table_id`, `database_name`, `schema_name`, `table_name` and `index_name` in `crdb_internal.ranges` and `.ranges_no_leases` will become meaningless, since a range will no longer be attributed to a single table/index. In v23.1, the default behavior of [`SHOW RANGES`]({% link v23.1/show-ranges.md %}) is retained, but you should consider setting the [`sql.show_ranges_deprecated_behavior.enabled`]({% link v23.1/cluster-settings.md %}#setting-sql-show-ranges-deprecated-behavior-enabled) cluster setting to `false`. This will have the following effects that will become the defaults in a future release:

   - The aforementioned columns in the `crdb_internal` virtual tables will be removed. Existing code can use the updated output of the [`SHOW RANGES`]({% link v23.1/show-ranges.md %}) statement instead, optionally using `WITH KEYS` to expose the raw start/end keys.
   - `SHOW RANGES FROM DATABASE` continues to report one row per range, but stops returning the database / schema / table / index name.
   - `SHOW RANGES FROM TABLE` continues to report one row per range, but stops returning the index name.  Suggested replacements:
      - Instead of: `SELECT range_id FROM crdb_internal.ranges WHERE table_name = 'x'`, use: `SELECT range_id FROM [SHOW RANGES FROM TABLE x]`
      - Instead of `SELECT range_id FROM crdb_internal.ranges WHERE table_name = $1 OR table_id = $2` (variable / unpredictable table name or ID), use: `SELECT range_id FROM [SHOW RANGES FROM CURRENT_CATALOG WITH TABLES] WHERE table_name = $1 OR table_id = $2`
      - Instead of `SELECT start_key FROM crdb_internal.ranges WHERE table_name = 'x'`, use: `SELECT raw_start_key FROM [SHOW RANGES FROM TABLE x WITH KEYS]`
      - Instead of `SELECT start_key FROM crdb_internal.ranges WHERE table_name = $1 OR table_id = $2` (unpredictable / variable table name or ID), use: `SELECT raw_start_key FROM [SHOW RANGES FROM CURRENT_CATALOG WITH TABLES, KEYS] WHERE table_name = $1 OR table_id = $2` [#93644][#93644]
- When the [cluster setting `sql.show_ranges_deprecated_behavior.enabled` is set to `false` (recommended in v23.1)]({% link v23.1/cluster-settings.md %}#setting-sql-show-ranges-deprecated-behavior-enabled), the format of the columns `start_key` and `end_key` for [`SHOW RANGES FROM DATABASE`]({% link v23.1/show-ranges.md %}) and `SHOW RANGES FROM TABLE` have been extended to include which table/index the key belongs to. This is necessary because a range can now contain data from more than one table/index. [#93644][#93644]
- When the [cluster setting `sql.show_ranges_deprecated_behavior.enabled` is set to `false` (recommended in v23.1)]({% link v23.1/cluster-settings.md %}#setting-sql-show-ranges-deprecated-behavior-enabled), the output of [`SHOW RANGES`]({% link v23.1/show-ranges.md %}) no longer includes `range_size`, `range_size_mb`, `lease_holder`, or `lease_holder_localities` by default. This ensures that `SHOW RANGES` remains fast in the common case. Use the new option [`WITH DETAILS`]({% link v23.1/show-ranges.md %}#options) to include these columns. [#93644][#93644]
- The format of the columns `start_key` and `end_key` for `SHOW RANGE ... FOR ROW` has been changed to be consistent with the output of `SHOW RANGES FROM INDEX`. [#93644][#93644]
- Changefeeds using "preview" expressions (released in v23.1.0) and that access the previous state of the row using the `cdc_prev()` function will no longer work and will need to be recreated with new syntax. [#94429][#94429]
- Some of the transformations specific to changefeeds have been deprecated and replaced. These functions were released in limited access in v22.2.  Deprecated changefeed transformations continue to function. Closely monitor changefeeds that are created during upgrade. While effort was made to maintain backward compatibility, the updated changefeed transformation may produce slightly different output, such as different column names. [#96295][#96295]
- Fixed a bug where, when `server.identity_map.configuration` was used, CockroachDB did not verify the client-provided username against the target mappings. Note that **this means that the client must now provide a valid DB username.** This requirement is compatible with PostgreSQL; it was not previously required by CockroachDB but it is now. This does not apply when identity maps are not in use. [#94915][#94915]
- Previously, the type of the `replicas`, `voting_replicas`,`non_voting_replicas` and `learner_replicas` in `crdb_internal.ranges` were overridden to `INT2VECTOR` causing incompatible indexing between `.ranges` and `.ranges_no_leases`. Now the types of those columns in the two tables are set to `INT[]`. [#96287][#96287]
- The output of the [`SHOW RANGES`]({% link v23.1/show-ranges.md %}) command for the `crdb_internal.ranges` and `crdb_internal.ranges_no_leases` tables has been updated, and the previous output is deprecated. To enable the new command output, set the `sql.show_ranges_deprecated_behavior.enabled` [cluster setting]({% link v23.1/cluster-settings.md %}) to `false`. The new output will become default in v23.2. [#99618][#99618]
- Previously, if a user specified a [`search_path`]({% link v23.1/sql-name-resolution.md %}#current-schema) in the connection string parameters, it would always be treated as case sensitive. Now, in order to have the schema names in the `search_path` respect case, the user must include double quotes around the name. [#101492][#101492]
- The deprecated CLI command `debug unsafe-remove-dead-replicas` has been removed. Use `debug recover` instead. [#89150][#89150]

<h4 id="v23-1-0-cluster-settings">Key Cluster Setting Changes</h4>

The following changes should be reviewed prior to upgrading. Default cluster settings will be used unless you have manually set a value for a setting. This can be confirmed by checking the `system.settings` table (`select * from system.settings`) to view the non-default settings.

| Category | Description | Change Type | Backport version |
|---|---|---|---|
| SQL language change | The [cluster setting]({% link v23.1/cluster-settings.md %}) `sql.ttl.default_range_concurrency` and table storage parameter `ttl_range_concurrency` are no longer configurable. [#89392](https://github.com/cockroachdb/cockroach/pull/89392) | No longer configurable | v22.2.1 |
| SQL language change | The `sql.distsql.max_running_flows` [cluster setting]({% link v23.1/cluster-settings.md %}) has been removed. [#84888](https://github.com/cockroachdb/cockroach/pull/84888) | Removed | None |
| Operational change | The [cluster settings]({% link v23.1/cluster-settings.md %}) `server.web_session.purge.period` and `server.web_session.purge.max_deletions_per_cycle`, which were specific to the cleanup function for `system.web_sessions`, have been replaced by `server.log_gc.period` and `server.log_gc.max_deletions_per_cycle` which apply to the cleanup function for `system.eventlog`, `system.rangelog` and `system.web_sessions` equally. [#90789](https://github.com/cockroachdb/cockroach/pull/90789) | Removed, repurposed | None |
| Operational change | The [cluster setting]({% link v23.1/cluster-settings.md %}) `server.web_session.auto_logout.timeout` has been removed. [#90789](https://github.com/cockroachdb/cockroach/pull/90789) | Removed, defaults to true | None |
| Operational change | The [load-based splitter]({% link v23.1/load-based-splitting.md %}) now supports using request CPU usage to split ranges. This is introduced with the previous cluster setting `kv.allocator.load_based_rebalancing.objective`, which when set to `cpu`, will use request CPU usage. The threshold above which CPU usage of a range is considered for splitting is defined in the cluster setting `kv.range_split.load_cpu_threshold`, which has a default value of `250ms`. (Relates to #100211 in this table.) [#96128](https://github.com/cockroachdb/cockroach/pull/96128) | Repurposed | None |
| Operational change | The `kv.range_split.load_cpu_threshold` [cluster setting]({% link v23.1/cluster-settings.md %}#setting-kv-range-split-load-cpu-threshold) now has a minimum setting value of `10ms`. Previously there was no minimum so, while unlikely, this could have an impact if you had chosen a custom setting lower than the established minimum. [#98250](https://github.com/cockroachdb/cockroach/pull/98250) | New minimum | None |
| Security update | The new [cluster setting]({% link v23.1/cluster-settings.md %}) `server.user_login.downgrade_scram_stored_passwords_to_bcrypt.enabled`, which allows you to migrate passwords from SCRAM to bcrypt during user authentication, defaults to `true`. If it is `true` and if `server.user_login.password_encryption` is `crdb-bcrypt`, then during login, the stored hashed password will be migrated from SCRAM to bcrypt. [#97429](https://github.com/cockroachdb/cockroach/pull/97429) | New setting | v22.2.6 |
| Security update | The default value for the `server.user_login.password_hashes.default_cost.scram_sha_256` [cluster setting]({% link v23.1/cluster-settings.md %}) is now 10610. (Previously the default was 119680.) The old value was found to have been too high for many types of client hardware, and in some cases could cause regressions in connection latency. The new value was chosen by running tests with clients that have 1 or 2 vCPUs provisioned. Additionally, the new cluster setting `server.user_login.rehash_scram_stored_passwords_on_cost_change.enabled` was added, and defaults to `true`. If it is `true` and the stored SCRAM password for a user has a different cost than the configured default cost, then the next time the user logs in, their password will automatically be rehashed using the configured default cost. If the rehashing is not desired, then operators should update the `server.user_login.password_hashes.default_cost.scram_sha_256` cluster setting to the value they desire before upgrading. This change is being backported to [v22.2](https://www.cockroachlabs.com/docs/releases/v22.2). [#98254](https://github.com/cockroachdb/cockroach/pull/98254) | Changed default | v22.2.7 |
| Command-line change | The `--drain-wait` argument to the [`cockroach node drain`]({% link v23.1/cockroach-node.md %}) command will be automatically increased if the command detects that it is smaller than the sum of the [cluster settings]({% link v23.1/node-shutdown.md %}#cluster-settings) `server.shutdown.drain_wait`, `server.shutdown.connection_wait`, `server.shutdown.query_wait` times two, and `server.shutdown.lease_transfer_wait`. If the `--drain-wait` argument is 0, then no timeout is used. This recommendation [was already documented]({% link v23.1/node-shutdown.md %}#drain-timeout), but now the advice will be applied automatically. [#98390](https://github.com/cockroachdb/cockroach/pull/98390) | New effect | v22.2.1 |
| Bug fix | RPC connections between nodes now require RPC connections to be established in both directions, otherwise the connection will be closed. This is done to prevent asymmetric network partitions where nodes are able to send outbound messages but not receive inbound messages, which could result in persistent unavailability. This behavior can be disabled by the [cluster setting]({% link v23.1/cluster-settings.md %}) `rpc.dialback.enabled`. [#94778](https://github.com/cockroachdb/cockroach/pull/94778) | New setting, enabled by default | None |
| Bug fix | Fixed a rare bug introduced in v22.2.0 that could cause a node to crash with an `attempting to append refresh spans after the tracked timestamp has moved forward` error when querying virtual tables in the [`crdb_internal`]({% link v23.1/crdb-internal.md %}) or [`pg_catalog`]({% link v23.1/pg-catalog.md %}) system catalogs. If you are experiencing this bug, set the `sql.distsql.use_streamer.enabled` [cluster setting]({% link v23.1/cluster-settings.md %}) to `false` before upgrading a cluster to v23.1. [#99443](https://github.com/cockroachdb/cockroach/pull/99443) | New guidance | v22.2.8 |
| Bug fix | The [**Hot Ranges** page]({% link v23.1/ui-hot-ranges-page.md %}) DB Console page would show hot ranges by CPU and not QPS (queries per second), depending on the value of the `kv.allocator.load_based_rebalancing.objective` [cluster setting]({% link v23.1/cluster-settings.md %}) (default `cpu`). Now the page will always collect statistics based on QPS. (Relates to #96128 in this table.) [#100211](https://github.com/cockroachdb/cockroach/pull/100211) | Repurposed setting | No |

<h4 id="v23-1-0-deprecations">Deprecations</h4>

- Ordinal column references (e.g., `SELECT @1, @2 FROM t`) are now deprecated. By default, statements using this syntax will now result in an error. If desired, such statements can be allowed using the session setting `SET allow_ordinal_column_references=true`. Support for ordinal column references is scheduled to be removed in upcoming version v23.2. [#93754][#93754]
- The `CONTROLCHANGEFEED` [role option]({% link v23.1/alter-role.md %}#role-options) will be deprecated in the future (see issue [#94757](https://github.com/cockroachdb/cockroach/issues/94757)). With this change, usages of the `CONTROLCHANGEFEED` role option will come with a deprecation warning. Its existing behavior remains the same. The `SELECT` and `CHANGEFEED` privileges will be used for changefeeds henceforth:
   - The `SELECT` privilege on a set of tables allows a user to run core changefeeds against them.
   - The `CHANGEFEED` privilege on a set of tables allows a user to run enterprise changefeeds on them, and also manage the underlying changefeed job (ie. view, pause, cancel, and resume the job).
   Notably, a new [cluster setting]({% link v23.1/cluster-settings.md %}) `changefeed.permissions.require_external_connection_sink.enabled` is added and set to `false` by default. Enabling this setting restricts users with `CHANGEFEED` on a set of tables to create enterprise changefeeds into external connections only. To use a given external connection, a user typically needs the `USAGE` privilege on it.  Note that `ALTER DEFAULT PRIVILEGES` can be used with both the `CHANGEFEED` and `SELECT` privileges to assign coarse-grained permissions (i.e., assign permissions to all tables in a schema rather than manually assign them for each table). [#94796][#94796]
- Deprecated the `PGDUMP` and `MYSQLDUMP` formats for [`IMPORT`]({% link v23.1/import.md %}). They are still present, but will be removed in a future release. See the [Migration Overview]({% link v23.1/migration-overview.md %}) page for alternatives. [#96386][#96386]

<h4 id="v23-1-0-known-limitations">Known limitations</h4>

For information about new and unresolved limitations in CockroachDB v23.1, with suggested workarounds where applicable, see [Known Limitations]({% link v23.1/known-limitations.md %}).

<h4 id="v23-1-0-additional-resources">Additional resources</h4>

Resource             | Topic                                      | Description
---------------------+--------------------------------------------+-------------
Cockroach University | [Introduction to Distributed SQL and CockroachDB](https://university.cockroachlabs.com/courses/course-v1:crl+intro-to-distributed-sql-and-cockroachdb+self-paced/about) | This course introduces the core concepts behind distributed SQL databases and describes how CockroachDB fits into this landscape. You will learn what differentiates CockroachDB from both legacy SQL and NoSQL databases and how CockroachDB ensures consistent transactions without sacrificing scale and resiliency. You'll learn about CockroachDB's seamless horizontal scalability, distributed transactions with strict ACID guarantees, and high availability and resilience.
Cockroach University | [Practical First Steps with CockroachDB](https://university.cockroachlabs.com/courses/course-v1:crl+practical-first-steps-with-crdb+self-paced/about) | This course will give you the tools you need to get started with CockroachDB. During the course, you will learn how to spin up a cluster, use the Admin UI to monitor cluster activity, and use SQL shell to solve a set of hands-on exercises.
Cockroach University | [Building a Highly Resilient Multi-region Database using CockroachDB](https://university.cockroachlabs.com/courses/course-v1:crl+intro-to-resilience-in-multi-region+self-paced/about) | This course is part of a series introducing solutions to running low-latency, highly resilient applications for data-intensive workloads on CockroachDB. In this course we focus on surviving large-scale infrastructure failures like losing an entire cloud region without losing data during recovery. We’ll show you how to use CockroachDB survival goals in a multi-region cluster to implement a highly resilient database that survives node or network failures across multiple regions with zero data loss.
Cockroach University | [Introduction to Serverless Databases and CockroachDB Serverless](https://university.cockroachlabs.com/courses/course-v1:crl+intro-to-serverless+self-paced/about) | This course introduces the core concepts behind serverless databases and gives you the tools you need to get started with CockroachDB Serverless. You will learn how serverless databases remove the burden of configuring, sizing, provisioning, securing, maintaining and dynamically scaling your database based on load. This means you simply pay for the serverless database resources you use.
Docs                 | [Migration Overview]({% link v23.1/migration-overview.md %}) | This page summarizes the steps of migrating a database to CockroachDB, which include testing and updating your schema to work with CockroachDB, moving your data into CockroachDB, and testing and updating your application.
Docs                 | [Developer Guide Overview]({% link v23.1/developer-guide-overview.md %}) | This page provides an overview of resources available to developers building applications on CockroachDB.
Docs                 | [Security Overview](https://www.cockroachlabs.com/docs/v23.1/security-reference/security-overview) | The 23.1 release encapsulates a number of security milestones. See the security overview for a summary.
Docs                 | [Architecture Overview](https://www.cockroachlabs.com/docs/v23.1/architecture/overview) | This page provides a starting point for understanding the architecture and design choices that enable CockroachDB's scalability and consistency capabilities.
Docs                 | [SQL Feature Support]({% link v23.1/sql-feature-support.md %}) | The page summarizes the standard SQL features CockroachDB supports as well as common extensions to the standard.
Docs                 | [Change Data Capture Overview]({% link v23.1/change-data-capture-overview.md %}) | This page summarizes CockroachDB's data streaming capabilities. Change data capture (CDC) provides efficient, distributed, row-level changefeeds into a configurable sink for downstream processing such as reporting, caching, or full-text indexing.
Docs                 | [Backup Architecture]({% link v23.1/backup-architecture.md %}) | This page describes the backup job workflow with a high-level overview, diagrams, and more details on each phase of the job.
