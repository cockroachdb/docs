Variable | Description
------|------------
`range_min_bytes` | <a name="range-min-bytes"></a> The minimum size, in bytes, for a range of data in the zone. When a range is less than this size, CockroachDB will merge it with an adjacent range.<br><br>**Default:** `134217728` (128 MiB)
`range_max_bytes` | <a name="range-max-bytes"></a> The maximum size, in bytes, for a [range]({{link_prefix}}architecture/glossary.html#architecture-range) of data in the zone. When a range reaches this size, CockroachDB will [split it]({{link_prefix}}architecture/distribution-layer.html#range-splits) into two ranges.<br><br>**Default:** `536870912` (512 MiB)
`gc.ttlseconds` | <a name="gc-ttlseconds"></a> The number of seconds overwritten [MVCC values]({{link_prefix}}architecture/storage-layer.html#mvcc) will be retained before [garbage collection]({{link_prefix}}architecture/storage-layer.html#garbage-collection). <br/><br/> **Default:** `14400` (4 hours) <br/><br/> Smaller values can save disk space and improve performance if values are frequently overwritten or for queue-like workloads. The smallest value we regularly test is `600` (10 minutes); smaller values are unlikely to be beneficial because of the frequency with which GC runs. If you use [non-scheduled incremental backups](take-full-and-incremental-backups.html#garbage-collection-and-backups), the GC TTL must be greater than the interval between incremental backups. Otherwise, your incremental backups will fail with [the error message `protected ts verification error`](common-errors.html#protected-ts-verification-error). To avoid this problem, we recommend using [scheduled backups](create-schedule-for-backup.html) instead, which automatically [use protected timestamps](create-schedule-for-backup.html#protected-timestamps-and-scheduled-backups) to ensure they succeed. <br/><br/> Larger values increase the interval allowed for [`AS OF SYSTEM TIME`](as-of-system-time.html) queries and allow for less frequent incremental backups. The largest value we regularly test is `90000` (25 hours). Increasing the GC TTL is not meant to be a solution for long-term retention of history; for that you should handle versioning in the [schema design at the application layer](schema-design-overview.html).  Setting the GC TTL too high can cause problems if the retained versions of a single row approach the [maximum range size](#range-max-bytes). This is important because all versions of a row are stored in a single range that never [splits](architecture/distribution-layer.html#range-splits). <br/><br/>
`num_replicas` | <a name="num_replicas"></a> The number of replicas in the zone, also called the "replication factor".<br><br>**Default:** `3`<br><br>For the `system` database and `.meta`, `.liveness`, and `.system` ranges, the default value is `5`.<br /><br />For [multi-region databases configured to survive region failures]({% link {{ page.version.version }}/multiregion-survival-goals.md %}#survive-region-failures), the default value is `5`; this will include both [voting](#num_voters) and [non-voting replicas]({% link {{ page.version.version }}/architecture/replication-layer.md %}#non-voting-replicas).
`constraints` | <a name="constraints"></a> An array of required (`+`) and/or prohibited (`-`) constraints influencing the location of replicas. See [Types of Constraints]({% link {{ page.version.version }}/configure-replication-zones.md %}#types-of-constraints) and [Scope of Constraints]({% link {{ page.version.version }}/configure-replication-zones.md %}#scope-of-constraints) for more details.<br/><br/>To prevent hard-to-detect typos, constraints placed on [store attributes and node localities]({% link {{ page.version.version }}/configure-replication-zones.md %}#descriptive-attributes-assigned-to-nodes) must match the values passed to at least one node in the cluster. If not, an error is signalled. To prevent this error, make sure at least one active node is configured to match the constraint. For example, apply `constraints = '[+region=west]'` only if you had set `--locality=region=west` for at least one node while starting the cluster.<br/><br/>**Default:** No constraints, with CockroachDB locating each replica on a unique node and attempting to spread replicas evenly across localities.
`lease_preferences` <a name="lease_preferences"></a> | An ordered list of required and/or prohibited constraints influencing the location of [leaseholders]({% link {{ page.version.version }}/architecture/glossary.md %}#architecture-leaseholder).  Whether each constraint is required or prohibited is expressed with a leading `+` or `-`, respectively.  Note that lease preference constraints do not have to be shared with the `constraints` field.  For example, it's valid for your configuration to define a `lease_preferences` field that does not reference any values from the `constraints` field.  It's also valid to define a `lease_preferences` field with no `constraints` field at all. <br /><br />  If the first preference cannot be satisfied, CockroachDB will attempt to satisfy the second preference, and so on.  If none of the preferences can be met, the lease will be placed using the default lease placement algorithm, which is to base lease placement decisions on how many leases each node already has, trying to make all the nodes have around the same amount.<br /><br />Each value in the list can include multiple constraints.  For example, the list `[[+zone=us-east-1b, +ssd], [+zone=us-east-1a], [+zone=us-east-1c, +ssd]]` means "prefer nodes with an SSD in `us-east-1b`, then any nodes in `us-east-1a`, then nodes in `us-east-1c` with an SSD."<br /><br /> For a usage example, see [Constrain leaseholders to specific availability zones]({% link {{ page.version.version }}/configure-replication-zones.md %}#constrain-leaseholders-to-specific-availability-zones).<br /><br />**Default**: No lease location preferences are applied if this field is not specified.
`global_reads` | <a name="global_reads"></a> If `true`, transactions operating on the range(s) affected by this zone config should be [non-blocking]({% link {{ page.version.version }}/architecture/transaction-layer.md %}#non-blocking-transactions), which slows down writes but allows reads from any replica in the range. Most users will not need to modify this setting; it is applied automatically when you [use the `GLOBAL` table locality in a multi-region cluster]({% link {{ page.version.version }}/global-tables.md %}).
`num_voters` | <a name="num_voters"></a> Specifies the number of [voting replicas]({% link {{ page.version.version }}/architecture/life-of-a-distributed-transaction.md %}#consensus). When set, `num_replicas` will be the sum of voting and [non-voting replicas]({% link {{ page.version.version }}/architecture/replication-layer.md %}#non-voting-replicas). Most users will not need to modify this setting; it is part of the underlying machinery that enables [improved multi-region capabilities in v21.1 and above]({% link {{ page.version.version }}/multiregion-overview.md %}).
`voter_constraints` | <a name="voter_constraints"></a> Specifies the constraints that govern the placement of voting replicas. This differs from the `constraints` field, which will govern the placement of all voting and non-voting replicas. Most users will not need to modify this setting; it is part of the underlying machinery that enables [improved multi-region capabilities in v21.1 and above]({% link {{ page.version.version }}/multiregion-overview.md %}).

{{site.data.alerts.callout_info}}
If a value is not set, new zone configurations will inherit their values from their parent zone (e.g., a partition zone inherits from the table zone), which is not necessarily `default`.

If a variable is set to `COPY FROM PARENT` (e.g., `range_max_bytes = COPY FROM PARENT`), the variable will copy its value from its parent [replication zone]({% link {{ page.version.version }}/configure-replication-zones.md %}). The `COPY FROM PARENT` value is a convenient shortcut to use so you do not have to look up the parent's current value. For example, the `range_max_bytes` and `range_min_bytes` variables must be set together, so when editing one value, you can use `COPY FROM PARENT` for the other. Note that if the variable in the parent replication zone is changed after the child replication zone is copied, the change will not be reflected in the child zone.
{{site.data.alerts.end}}
