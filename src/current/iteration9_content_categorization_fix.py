#!/usr/bin/env python3
"""
ITERATION 9: Content Categorization Fix
Fix ranking by improving content categorization and extraction patterns
No hardcoded production data - purely based on content analysis
"""

import os
import logging
import hashlib
import json
import re
from pathlib import Path
from bs4 import BeautifulSoup
import asyncio
from algoliasearch.search.client import SearchClient

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ContentCategorizationFix:
    def __init__(self):
        self.setup_clients()
        self.site_path = Path("/Users/eeshan/Desktop/docs/src/current/_site/docs")
        
    def setup_clients(self):
        """Setup Algolia clients"""
        app_id = os.environ.get('ALGOLIA_APP_ID', '7RXZLDVR5F')
        admin_key = os.environ.get('ALGOLIA_STAGE_ADMIN_KEY')
        read_key = os.environ.get('ALGOLIA_PROD_READ_KEY')
        
        if not admin_key or not read_key:
            raise ValueError("Missing API keys")
            
        self.stage_client = SearchClient(app_id, admin_key)
        self.prod_client = SearchClient(app_id, read_key)
    
    def analyze_content_for_categorization(self, html_file):
        """Analyze content to determine better categorization"""
        try:
            with open(html_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            
            # Extract all text for analysis
            main = soup.find('main') or soup.find('body')
            if main:
                # Remove nav elements but keep content
                for elem in main.find_all(['nav', 'aside', 'footer', 'header', 'script', 'style']):
                    elem.decompose()
                
                full_text = main.get_text().lower()
            else:
                full_text = soup.get_text().lower()
            
            file_path_lower = str(html_file).lower()
            
            # Analyze content patterns for better categorization
            content_signals = {
                # Getting started signals
                'is_getting_started': any(signal in full_text or signal in file_path_lower for signal in [
                    'spatial data overview', 'overview', 'introduction', 'before you begin',
                    'kubernetes', 'deploy', 'installation', 'quick start', 'get started',
                    'ccloud cli', 'free trial', 'first time'
                ]),
                
                # SQL reference signals  
                'is_sql_reference': any(signal in full_text or signal in file_path_lower for signal in [
                    'alter table', 'create table', 'sql statement', 'syntax', 'parameters',
                    'examples', 'sql command', 'statement', 'reference'
                ]),
                
                # SQL context/related signals
                'is_sql_context': any(signal in full_text or signal in file_path_lower for signal in [
                    'schema changes', 'online schema', 'event types', 'limitations', 'audit logging',
                    'table-based', 'notable event', 'known limitation'
                ]),
                
                # Cluster management signals
                'is_cluster_management': any(signal in full_text or signal in file_path_lower for signal in [
                    'cluster settings', 'cluster overview', 'cluster monitoring', 'cluster sso',
                    'cluster management', 'admin', 'administration', 'manage cluster'
                ]),
                
                # Tutorial/app building signals
                'is_tutorial': any(signal in full_text or signal in file_path_lower for signal in [
                    'build a', 'app with cockroachdb', 'tutorial', 'example app', 'sample app'
                ])
            }
            
            return content_signals
            
        except Exception as e:
            return {}
    
    def extract_content_with_improved_categorization(self, html_file):
        """Extract content with improved categorization for ranking"""
        try:
            with open(html_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            
            # Get content signals
            signals = self.analyze_content_for_categorization(html_file)
            
            # IMPROVED TITLE EXTRACTION
            title = None
            
            # Jekyll front matter first
            title_match = re.search(r'title:\s*["\']?([^"\'>\n]+)["\']?', content)
            if title_match:
                title = title_match.group(1).strip()
            
            if not title:
                # Look for main heading
                h1 = soup.find('h1')
                if h1:
                    title = h1.get_text().strip()
                    title = re.sub(r'Edit.*?$', '', title)
            
            if not title:
                title_elem = soup.find('title')
                if title_elem:
                    title = title_elem.get_text().strip()
                    title = re.sub(r'\s*[\|\-]\s*CockroachDB.*', '', title)
            
            if not title or len(title) < 3:
                title = html_file.stem.replace('-', ' ').title()
            
            # CONTENT EXTRACTION with categorization awareness
            main_content = soup.find('main') or soup.find('body')
            if main_content:
                # Remove UI elements
                for elem in main_content.find_all(['nav', 'aside', 'footer', 'header', 'script', 'style']):
                    elem.decompose()
                
                # Get text content
                content_text = main_content.get_text(separator=' ')
                
                # For getting started content, prioritize overview/intro text
                if signals.get('is_getting_started'):
                    # Look for overview paragraphs
                    overview_text = ""
                    for p in main_content.find_all('p'):
                        p_text = p.get_text().strip()
                        if any(keyword in p_text.lower() for keyword in ['overview', 'introduction', 'before you begin', 'this guide']):
                            overview_text += p_text + " "
                    
                    if overview_text:
                        content_text = overview_text + content_text[:1000]
                
                # For SQL reference, prioritize syntax and examples
                if signals.get('is_sql_reference') or signals.get('is_sql_context'):
                    # Look for syntax sections
                    syntax_text = ""
                    for elem in main_content.find_all(['code', 'pre']):
                        syntax_text += elem.get_text() + " "
                    
                    if syntax_text:
                        content_text = syntax_text[:500] + " " + content_text[:1500]
                
                content_html = str(main_content)
            else:
                content_text = soup.get_text()
                content_html = str(soup)
            
            # Clean content
            content_text = re.sub(r'Version v\d+\.\d+.*', '', content_text)
            content_text = re.sub(r'Contribute.*?Report Doc.*', '', content_text, flags=re.DOTALL)
            content_text = ' '.join(content_text.split())
            
            # IMPROVED SUMMARY based on content type
            summary = None
            
            if main_content:
                # For different content types, extract different summary patterns
                if signals.get('is_getting_started'):
                    # Look for overview/intro paragraphs
                    for p in main_content.find_all('p'):
                        p_text = p.get_text().strip()
                        if (len(p_text) > 50 and 
                            any(keyword in p_text.lower() for keyword in ['overview', 'this page', 'before you begin', 'introduction'])):
                            summary = p_text[:250] + ('...' if len(p_text) > 250 else '')
                            break
                
                elif signals.get('is_sql_reference'):
                    # For SQL docs, use description of the command
                    for p in main_content.find_all('p'):
                        p_text = p.get_text().strip()
                        if (len(p_text) > 30 and 
                            any(keyword in p_text.lower() for keyword in ['statement', 'command', 'syntax', 'use', 'allows you'])):
                            summary = p_text[:250] + ('...' if len(p_text) > 250 else '')
                            break
                
                elif signals.get('is_cluster_management'):
                    # For cluster docs, focus on management aspects
                    for p in main_content.find_all('p'):
                        p_text = p.get_text().strip()
                        if (len(p_text) > 50 and 
                            any(keyword in p_text.lower() for keyword in ['cluster', 'manage', 'configure', 'monitor'])):
                            summary = p_text[:250] + ('...' if len(p_text) > 250 else '')
                            break
                
                # Fallback to first good paragraph
                if not summary:
                    for p in main_content.find_all('p'):
                        p_text = p.get_text().strip()
                        if (len(p_text) > 50 and 
                            not re.match(r'^Version|^Contribute|^Edit|^Report', p_text)):
                            summary = p_text[:200] + ('...' if len(p_text) > 200 else '')
                            break
            
            if not summary:
                summary = content_text[:200] + '...'
            
            # Size control
            max_content = 20000
            max_html = 15000
            
            if len(content_text) > max_content:
                content_text = content_text[:max_content] + '...'
            if len(content_html) > max_html:
                content_html = content_html[:max_html] + '...'
            
            # Extract meaningful headings based on content type
            headings = []
            if main_content:
                for h in main_content.find_all(['h1', 'h2', 'h3'])[:8]:
                    h_text = h.get_text().strip()
                    h_text = re.sub(r'Edit.*?$', '', h_text)
                    if h_text and len(h_text) > 3 and len(h_text) < 100:
                        headings.append(h_text)
            
            return {
                'title': title,
                'content': content_text,
                'html': content_html,
                'summary': summary,
                'headings': headings,
                'signals': signals
            }
            
        except Exception as e:
            logger.error(f"Error extracting from {html_file}: {e}")
            return None
    
    def improve_docs_area_classification(self, file_path, content_signals, title, content):
        """Improve docs_area classification based on content analysis"""
        
        path_str = str(file_path).lower()
        title_lower = title.lower()
        content_lower = content.lower()[:1000]  # First 1000 chars
        
        # Enhanced classification based on content signals
        
        # Getting started / overview content
        if (content_signals.get('is_getting_started') or
            any(term in path_str for term in ['overview', 'introduction', 'getting-started', 'quickstart']) or
            any(term in title_lower for term in ['overview', 'introduction', 'getting started', 'spatial data overview'])):
            return 'get-started'
        
        # CockroachCloud content
        if ('cockroachcloud' in path_str or 'cloud' in path_str or 
            any(term in title_lower for term in ['cockroachdb cloud', 'ccloud', 'cloud'])):
            return 'cockroachcloud'
        
        # SQL reference and context
        if (content_signals.get('is_sql_reference') or content_signals.get('is_sql_context') or
            any(term in path_str for term in ['sql', 'alter', 'create', 'select', 'insert']) or
            any(term in title_lower for term in ['alter table', 'create table', 'sql', 'statement']) or
            any(term in content_lower for term in ['sql statement', 'syntax', 'command'])):
            
            # Sub-categorize SQL content
            if any(term in path_str or term in title_lower for term in ['event', 'logging', 'audit']):
                return 'reference.logging'
            elif any(term in path_str or term in title_lower for term in ['schema', 'online schema']):
                return 'reference.schema'
            elif any(term in path_str or term in title_lower for term in ['limitation', 'known limitation']):
                return 'releases'
            else:
                return 'reference.sql'
        
        # Cluster management
        if (content_signals.get('is_cluster_management') or
            any(term in path_str for term in ['cluster', 'admin', 'manage', 'monitor']) or
            any(term in title_lower for term in ['cluster', 'monitoring', 'management']) or
            any(term in content_lower for term in ['cluster settings', 'manage cluster', 'monitor cluster'])):
            
            # Sub-categorize cluster content
            if any(term in title_lower or term in content_lower for term in ['sso', 'authentication', 'sign-on']):
                return 'secure'
            elif any(term in title_lower or term in content_lower for term in ['monitoring', 'monitor']):
                return 'manage'
            else:
                return 'reference.cluster_settings'
        
        # Deployment content
        if (any(term in path_str for term in ['deploy', 'kubernetes', 'installation']) or
            any(term in title_lower for term in ['deploy', 'kubernetes', 'orchestrate']) or
            any(term in content_lower for term in ['deployment', 'kubernetes', 'install'])):
            return 'deploy'
        
        # Management content
        if (any(term in path_str for term in ['backup', 'restore', 'performance', 'security']) or
            any(term in title_lower for term in ['backup', 'restore', 'performance', 'security'])):
            return 'manage'
        
        # Default
        return 'reference'
    
    def create_improved_categorization_record(self, html_file, existing_production_paths):
        """Create record with improved categorization"""
        
        file_path = Path(html_file)
        
        try:
            rel_path = file_path.relative_to(self.site_path)
        except ValueError:
            return None
        
        # Only process files that exist in production
        if str(rel_path) not in existing_production_paths:
            return None
        
        # Extract content with improved analysis
        content_data = self.extract_content_with_improved_categorization(html_file)
        if not content_data:
            return None
        
        # Improved docs_area classification
        docs_area = self.improve_docs_area_classification(
            rel_path, 
            content_data['signals'], 
            content_data['title'],
            content_data['content']
        )
        
        # Generate metadata
        version = 'v25.3'
        parts = rel_path.parts
        for part in parts:
            if part.startswith('v') and '.' in part:
                version = part
                break
        
        # URLs
        url_path = str(rel_path)
        canonical = f"/{url_path}"
        full_url = f"https://www.cockroachlabs.com/docs/{url_path}"
        
        # Object ID
        title = content_data['title']
        object_id = hashlib.md5(f"{title}_{url_path}".encode()).hexdigest()
        
        # Create record with improved categorization
        record = {
            'objectID': object_id,
            'title': title,
            'content': content_data['content'],
            'html': content_data['html'],
            'summary': content_data['summary'],
            'url': full_url,
            'canonical': canonical,
            'type': 'page',
            'version': version,
            'doc_type': 'cockroachdb',
            'docs_area': docs_area,  # Improved classification
            'slug': file_path.stem,
            'last_modified_at': '29-Aug-25',
            'excerpt_html': f"<p>{content_data['summary']}</p>",
            'excerpt_text': content_data['summary'],
            'headings': content_data['headings'],
            'tags': [],
            'categories': []
        }
        
        # Verify size
        size = len(json.dumps(record).encode('utf-8'))
        if size > 45000:
            # Truncate more aggressively
            record['content'] = record['content'][:15000] + '...'
            record['html'] = record['html'][:8000] + '...'
        
        return record
    
    async def get_current_production_paths(self):
        """Get production paths from current working index"""
        logger.info("üìÅ Getting current production paths...")
        
        paths = set()
        
        # Get a reasonable sample of production paths
        for page in range(5):  # First 5K records
            try:
                results = await self.prod_client.search_single_index("cockroachcloud_docs", {
                    "query": "",
                    "hitsPerPage": 1000,
                    "page": page,
                    "attributesToRetrieve": ["url"]
                })
                
                if not results.hits:
                    break
                
                for hit in results.hits:
                    try:
                        hit_dict = hit.model_dump()
                        url = hit_dict.get('url', '')
                        if url and '/docs/' in url:
                            path = url.split('/docs/')[-1]
                            paths.add(path)
                    except:
                        continue
                
                if len(results.hits) < 1000:
                    break
                    
            except Exception as e:
                logger.error(f"Error getting paths page {page}: {e}")
                break
        
        logger.info(f"‚úÖ Got {len(paths)} production paths")
        return paths
    
    async def reindex_with_improved_categorization(self):
        """Reindex with improved content categorization"""
        logger.info("üéØ ITERATION 9: IMPROVED CONTENT CATEGORIZATION")
        
        # Get production paths
        production_paths = await self.get_current_production_paths()
        
        # Process files
        all_files = list(self.site_path.rglob('*.html'))
        logger.info(f"üìÅ Processing {len(all_files)} HTML files...")
        
        records = []
        
        for i, html_file in enumerate(all_files):
            record = self.create_improved_categorization_record(html_file, production_paths)
            if record:
                records.append(record)
            
            if (i + 1) % 500 == 0:
                logger.info(f"   Processed {i + 1}/{len(all_files)} ({len(records)} valid)")
        
        logger.info(f"üìù Created {len(records)} improved records")
        
        # Clear and reindex
        await self.stage_client.clear_objects(index_name="stage_cockroach_docs")
        logger.info("üßπ Cleared stage index")
        
        # Upload in small batches
        batch_size = 20
        uploaded = 0
        failed = 0
        
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            try:
                await self.stage_client.save_objects(index_name="stage_cockroach_docs", objects=batch)
                uploaded += len(batch)
                
                if uploaded % 100 == 0:
                    logger.info(f"üì§ Uploaded {uploaded}/{len(records)}")
                    
            except Exception as e:
                logger.error(f"Batch failed, trying individual uploads...")
                for record in batch:
                    try:
                        await self.stage_client.save_objects(index_name="stage_cockroach_docs", objects=[record])
                        uploaded += 1
                    except:
                        failed += 1
        
        logger.info(f"‚úÖ Upload complete: {uploaded} successful, {failed} failed")
        return uploaded
    
    async def test_categorization_improvements(self):
        """Test if improved categorization fixed poor queries"""
        logger.info("üîç TESTING CATEGORIZATION IMPROVEMENTS")
        
        await asyncio.sleep(30)  # Wait for indexing
        
        test_queries = [
            'getting started', 'alter table', 'cluster', 'create table',
            'backup', 'authentication', 'performance'  # controls
        ]
        
        results = {}
        poor_query_improvements = {}
        
        for query in test_queries:
            try:
                prod = await self.prod_client.search_single_index("cockroachcloud_docs", {"query": query, "hitsPerPage": 5})
                stage = await self.stage_client.search_single_index("stage_cockroach_docs", {"query": query, "hitsPerPage": 5})
                
                prod_titles = [hit.title for hit in prod.hits]
                stage_titles = [hit.title for hit in stage.hits]
                
                matches = sum(1 for t in stage_titles if t in prod_titles)
                similarity = matches / len(prod_titles) * 100 if prod_titles else 0
                
                results[query] = similarity
                
                if query in ['getting started', 'alter table', 'cluster', 'create table']:
                    poor_query_improvements[query] = similarity
                
                status = "üéâ" if similarity >= 80 else "‚úÖ" if similarity >= 70 else "‚ö†Ô∏è"
                logger.info(f"{status} '{query}': {similarity:.0f}% ({matches}/{len(prod_titles)})")
                
            except Exception as e:
                logger.error(f"Error testing '{query}': {e}")
        
        avg_similarity = sum(results.values()) / len(results) if results else 0
        poor_fixed = sum(1 for s in poor_query_improvements.values() if s >= 70)
        
        logger.info(f"\nüéØ CATEGORIZATION IMPROVEMENT RESULTS:")
        logger.info(f"   Average similarity: {avg_similarity:.1f}%")
        logger.info(f"   Poor queries ‚â•70%: {poor_fixed}/4")
        
        target_achieved = poor_fixed >= 3
        
        if target_achieved:
            logger.info("üéâ SUCCESS: Categorization fixes achieved target!")
        else:
            logger.info("üìä Partial improvement made")
        
        return results, avg_similarity, target_achieved

async def main():
    """Run improved categorization iteration"""
    try:
        fixer = ContentCategorizationFix()
        
        # Reindex with improved categorization
        record_count = await fixer.reindex_with_improved_categorization()
        
        # Test improvements
        results, avg_sim, success = await fixer.test_categorization_improvements()
        
        logger.info(f"\n‚úÖ ITERATION 9 COMPLETE:")
        logger.info(f"   Records: {record_count}")
        logger.info(f"   Average similarity: {avg_sim:.1f}%")
        logger.info(f"   Target achieved: {success}")
        
    except Exception as e:
        logger.error(f"‚ùå Iteration 9 failed: {e}")

if __name__ == '__main__':
    asyncio.run(main())